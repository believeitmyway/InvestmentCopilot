import json
import logging
import math
import os
import re
import time
from datetime import datetime, timedelta, timezone
from typing import Dict, List, Optional

import streamlit as st
import yfinance as yf
try:
    from ddgs import DDGS
except ImportError:
    # å¾Œæ–¹äº’æ›æ€§ã®ãŸã‚ã€å¤ã„ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸åã‚‚è©¦è¡Œ
    from duckduckgo_search import DDGS
from openai import OpenAI
import plotly.graph_objects as go
import plotly.express as px

try:
    import requests
    from bs4 import BeautifulSoup
    SCRAPING_AVAILABLE = True
except ImportError:
    SCRAPING_AVAILABLE = False

try:
    import google.generativeai as genai
except ImportError:  # pragma: no cover - optional dependency
    genai = None


st.set_page_config(
    page_title="Mobile-First AI Investment Dashboard",
    layout="centered",
    page_icon="ğŸ“ˆ",
)


MOBILE_CSS = """
<style>
    .stApp {
        background-color: #0f1116;
        color: #f3f4f6;
        font-family: "Inter", "Noto Sans JP", sans-serif;
    }
    section.main > div {
        padding-left: 12px;
        padding-right: 12px;
    }
    .header-card {
        position: sticky;
        top: 0;
        z-index: 900;
        background: #111827;
        padding: 18px 16px;
        border-radius: 18px;
        border: 1px solid #1f2937;
        box-shadow: 0 15px 30px rgba(0,0,0,0.25);
        margin-bottom: 18px;
    }
    .header-symbol {
        font-size: 0.9rem;
        letter-spacing: 0.08em;
        color: #9ca3af;
        text-transform: uppercase;
    }
    .header-price {
        font-size: 2.2rem;
        font-weight: 600;
        color: #f9fafb;
    }
    .price-change {
        font-size: 1rem;
        margin-top: 4px;
    }
    .price-change.positive { color: #10b981; }
    .price-change.negative { color: #f87171; }
    .score-grid {
        display: grid;
        grid-template-columns: 1fr 1fr;
        gap: 10px;
        margin-top: 16px;
    }
    .score-card {
        background: #0b1220;
        border-radius: 14px;
        padding: 12px;
        border: 1px solid #1f2937;
    }
    .score-label {
        font-size: 0.75rem;
        text-transform: uppercase;
        letter-spacing: 0.08em;
        color: #9ca3af;
    }
    .score-value {
        font-size: 1.4rem;
        font-weight: 600;
        margin-top: 4px;
    }
    .conclusion-card {
        background: #111827;
        border-radius: 18px;
        padding: 20px 18px;
        border: 1px solid #1f2937;
        margin-bottom: 18px;
    }
    .action-pill {
        display: inline-flex;
        align-items: center;
        padding: 6px 13px;
        border-radius: 999px;
        font-size: 0.85rem;
        font-weight: 600;
        text-transform: uppercase;
        letter-spacing: 0.08em;
    }
    .action-Buy { background: rgba(16,185,129,0.15); color: #10b981; }
    .action-Sell { background: rgba(248,113,113,0.15); color: #f87171; }
    .action-Hold { background: rgba(251,191,36,0.15); color: #fbbf24; }
    .bullet-list li {
        margin-bottom: 4px;
    }
    .news-item {
        padding: 10px 0;
        border-bottom: 1px solid #1f2937;
    }
    .news-item:last-child { border-bottom: none; }
    .news-title {
        font-weight: 600;
        color: #d1d5db;
    }
    .news-meta {
        font-size: 0.8rem;
        color: #9ca3af;
    }
    .tabs-container [data-baseweb="tab-list"] button {
        background: transparent;
        border: none;
        color: #9ca3af;
        font-weight: 600;
    }
    .tabs-container [aria-selected="true"] {
        color: #f3f4f6 !important;
        border-bottom: 2px solid #3b82f6;
    }
    .metric-stack {
        display: flex;
        flex-direction: column;
        gap: 8px;
    }
    .metric-row {
        display: flex;
        justify-content: space-between;
        font-size: 0.95rem;
        padding: 8px 0;
        border-bottom: 1px solid #1f2937;
    }
    .metric-row:last-child { border-bottom: none; }
    .disclaimer {
        font-size: 0.78rem;
        color: #6b7280;
        margin-top: 24px;
    }
      .api-status-panel {
          background: #111827;
          border-radius: 14px;
          padding: 16px;
          border: 1px solid #1f2937;
          margin-top: 18px;
      }
      .api-status-row {
          display: flex;
          justify-content: space-between;
          font-size: 0.9rem;
          padding: 6px 0;
          border-bottom: 1px solid #1f2937;
      }
      .api-status-row:last-child { border-bottom: none; }
      .api-status-value {
          font-weight: 600;
      }
      .api-status-value.active { color: #10b981; }
      .api-status-value.inactive { color: #f87171; }
    @media (min-width: 768px) {
        .header-card, .conclusion-card {
            margin-left: auto;
            margin-right: auto;
            max-width: 520px;
        }
        .score-grid {
            grid-template-columns: repeat(2, minmax(0, 1fr));
        }
    }
</style>
"""

st.markdown(MOBILE_CSS, unsafe_allow_html=True)


def load_prompt_file(filename: str, default: str = "") -> str:
    """ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€"""
    prompt_dir = os.path.join(os.path.dirname(__file__), "prompts")
    filepath = os.path.join(prompt_dir, filename)
    try:
        with open(filepath, "r", encoding="utf-8") as f:
            return f.read().strip()
    except FileNotFoundError:
        print(f"è­¦å‘Š: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {filepath}ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
        return default
    except Exception as e:
        print(f"è­¦å‘Š: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼ ({filepath}): {e}ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
        return default


def load_system_prompt() -> str:
    """ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’èª­ã¿è¾¼ã‚€"""
    default = (
        "You are an equity strategist who writes concise Japanese summaries for busy executives. "
        "Use the provided market snapshot to compare quantitative signals with analyst consensus. "
        "Output JSON exactly with the requested schema."
    )
    return load_prompt_file("system_prompt.txt", default)


def load_user_prompt_template() -> str:
    """ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’èª­ã¿è¾¼ã‚€"""
    default = (
        "ãƒãƒ¼ã‚±ãƒƒãƒˆãƒ‡ãƒ¼ã‚¿:\n"
        "{market_data}\n\n"
        "å‡ºåŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ(JSON):\n"
        "{\n"
        '"verdict_short":"",\n'
        '"action":"Buy | Sell | Hold",\n'
        '"score":0,\n'
        '"bullet_points":["","", ""],\n'
        '"scenario":{"bullish_case":"","bearish_case":"","competitive_edge":""},\n'
        '"analysis_comment":""\n'
        "}"
    )
    return load_prompt_file("user_prompt_template.txt", default)


def load_news_search_config() -> Dict:
    """ãƒ‹ãƒ¥ãƒ¼ã‚¹æ¤œç´¢è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€"""
    config_dir = os.path.join(os.path.dirname(__file__), "config")
    filepath = os.path.join(config_dir, "news_search_config.json")
    
    default_config = {
        "search": {
            "max_results": 15,
            "min_required_results": 5,
            "max_retries": 3,
            "retry_delay_seconds": 2,
            "multipliers": {
                "initial_japanese": 8,
                "fallback_japanese": 4,
                "english": 5
            },
            "min_candidates": {
                "initial_japanese": 50,
                "fallback_japanese": 30,
                "english": 30
            },
            "timeout": 30,
            "article_fetch_timeout": 15
        },
        "keywords": {
            "japanese_search_templates": [
                "{company_name} æ±ºç®— æ¥­ç¸¾",
                "{company_name} æ±ºç®—ç™ºè¡¨",
                "{company_name} æ¥­ç¸¾ç™ºè¡¨",
                "{company_name} IR æŠ•è³‡å®¶å‘ã‘èª¬æ˜ä¼š",
                "{company_name} æ ªä¸»ç·ä¼š",
                "{company_name} M&A è²·å åˆä½µ",
                "{company_name} å¤§å‹æŠ•è³‡ æˆ¦ç•¥ç™ºè¡¨",
                "{company_name} æ ªä¾¡ ãƒ‹ãƒ¥ãƒ¼ã‚¹",
                "{company_name} æ ª æœ€æ–°",
                "{company_name} ä¼æ¥­ ãƒ‹ãƒ¥ãƒ¼ã‚¹",
                "{company_name} æœ€æ–°ãƒ‹ãƒ¥ãƒ¼ã‚¹"
            ],
            "japanese_symbol_templates": [
                "{symbol} æ ªä¾¡",
                "{symbol} ãƒ‹ãƒ¥ãƒ¼ã‚¹",
                "{symbol} æ±ºç®—",
                "{symbol} æ¥­ç¸¾"
            ],
            "japanese_combined_templates": [
                "{symbol} {company_name}",
                "{company_name} {symbol}"
            ],
            "english_search_templates": [
                "{query} earnings results",
                "{query} quarterly results",
                "{query} financial results",
                "{query} acquisition merger",
                "{query} strategic announcement",
                "{query} stock news",
                "{query} stock"
            ]
        },
        "scoring": {
            "focus_score": {
                "company_name_in_title": 10,
                "company_name_in_snippet": 5,
                "company_name_count_multiplier": 2,
                "company_name_count_max": 10,
                "symbol_in_title": 8,
                "symbol_in_snippet": 4,
                "symbol_count_multiplier": 2,
                "symbol_count_max": 8,
                "query_in_title": 6,
                "query_in_snippet": 3,
                "deep_analysis_bonus": 2
            },
            "importance_score": {
                "keyword_score": 2
            }
        },
        "keywords_for_scoring": {
            "shallow_article": {
                "japanese": [
                    "ãƒ©ãƒ³ã‚­ãƒ³ã‚°", "ãƒˆãƒƒãƒ—", "ä¸Šä½", "ãƒ™ã‚¹ãƒˆ", "ãƒ¯ãƒ¼ã‚¹ãƒˆ",
                    "å¸‚å ´å‹•å‘", "ç›¸å ´æ¦‚æ³", "å¸‚æ³", "ãƒãƒ¼ã‚±ãƒƒãƒˆã‚µãƒãƒªãƒ¼",
                    "æ ªä¾¡ãƒ©ãƒ³ã‚­ãƒ³ã‚°", "ä¸Šæ˜‡ãƒ©ãƒ³ã‚­ãƒ³ã‚°", "ä¸‹è½ãƒ©ãƒ³ã‚­ãƒ³ã‚°",
                    "æ³¨ç›®éŠ˜æŸ„", "äººæ°—éŠ˜æŸ„", "æ€¥é¨°éŠ˜æŸ„", "æ€¥è½éŠ˜æŸ„",
                    "æ—¥çµŒå¹³å‡", "TOPIX", "ãƒ€ã‚¦å¹³å‡", "ãƒŠã‚¹ãƒ€ãƒƒã‚¯",
                    "å¸‚å ´ç·æ‹¬", "ç›¸å ´ç·æ‹¬", "å¸‚æ³ãƒ¬ãƒãƒ¼ãƒˆ",
                    "è¤‡æ•°éŠ˜æŸ„", "å¤šæ•°éŠ˜æŸ„", "å„éŠ˜æŸ„", "å„ç¤¾"
                ],
                "english": [
                    "ranking", "top", "best", "worst", "list",
                    "market overview", "market summary", "market wrap",
                    "stock ranking", "gainers", "losers", "most active",
                    "market movers", "market recap", "daily wrap",
                    "multiple stocks", "several stocks", "various stocks"
                ]
            },
            "important": {
                "japanese": [
                    "æ±ºç®—", "æ¥­ç¸¾", "æ¥­ç¸¾ç™ºè¡¨", "æ±ºç®—ç™ºè¡¨", "æ±ºç®—èª¬æ˜ä¼š",
                    "ir", "æŠ•è³‡å®¶å‘ã‘èª¬æ˜ä¼š", "æ ªä¸»ç·ä¼š",
                    "m&a", "è²·å", "åˆä½µ", "çµ±åˆ", "ææº",
                    "å¤§å‹æŠ•è³‡", "æˆ¦ç•¥ç™ºè¡¨", "çµŒå–¶æ–¹é‡", "ä¸­æœŸçµŒå–¶è¨ˆç”»",
                    "ä¸Šå ´", "ipo", "å¢—è³‡", "æ¸›è³‡", "é…å½“",
                    "ä¸ç¥¥äº‹", "ã‚³ãƒ³ãƒ—ãƒ©ã‚¤ã‚¢ãƒ³ã‚¹", "ãƒªã‚³ãƒ¼ãƒ«"
                ],
                "english": [
                    "earnings", "quarterly", "annual", "results", "financial results",
                    "acquisition", "merger", "m&a", "partnership",
                    "ipo", "dividend", "buyback", "strategic",
                    "recall", "scandal", "compliance"
                ]
            },
            "deep_analysis": {
                "japanese": [
                    "æˆ¦ç•¥", "çµŒå–¶æ–¹é‡", "ä¸­æœŸçµŒå–¶è¨ˆç”»", "äº‹æ¥­æˆ¦ç•¥",
                    "æ¥­ç¸¾åˆ†æ", "è²¡å‹™åˆ†æ", "æŠ•è³‡åˆ¤æ–­", "æŠ•è³‡è©•ä¾¡",
                    "ç«¶äº‰åŠ›", "ç«¶åˆåˆ†æ", "å¸‚å ´ã‚·ã‚§ã‚¢", "äº‹æ¥­å±•é–‹",
                    "IRèª¬æ˜ä¼š", "æ±ºç®—èª¬æ˜ä¼š", "æŠ•è³‡å®¶èª¬æ˜ä¼š"
                ],
                "english": [
                    "strategy", "business plan", "financial analysis",
                    "investment thesis", "competitive", "market share",
                    "earnings call", "investor day", "analyst meeting"
                ]
            }
        },
        "filtering": {
            "date_threshold_days": 365,
            "shallow_article": {
                "min_stock_codes": 3
            },
            "focus_score": {
                "min_focus_score": 0,
                "min_importance_score_when_focus_zero": 4
            },
            "fallback_sufficient_threshold_multiplier": 2
        }
    }
    
    try:
        with open(filepath, "r", encoding="utf-8") as f:
            loaded_config = json.load(f)
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã¨æ·±ã„ãƒãƒ¼ã‚¸ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ã«ãªã„é …ç›®ã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚’ä½¿ç”¨ï¼‰
            def deep_merge(default: Dict, loaded: Dict) -> Dict:
                """æ·±ã„ãƒãƒ¼ã‚¸ã‚’è¡Œã†ï¼ˆãƒã‚¹ãƒˆã•ã‚ŒãŸè¾æ›¸ã‚‚ãƒãƒ¼ã‚¸ï¼‰"""
                result = default.copy()
                for key, value in loaded.items():
                    if key in result and isinstance(result[key], dict) and isinstance(value, dict):
                        result[key] = deep_merge(result[key], value)
                    else:
                        result[key] = value
                return result
            return deep_merge(default_config, loaded_config)
    except FileNotFoundError:
        print(f"è­¦å‘Š: è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {filepath}ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
        return default_config
    except json.JSONDecodeError as e:
        print(f"è­¦å‘Š: è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®JSONè§£æã‚¨ãƒ©ãƒ¼ ({filepath}): {e}ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
        return default_config
    except Exception as e:
        print(f"è­¦å‘Š: è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼ ({filepath}): {e}ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
        return default_config


AI_SYSTEM_PROMPT = load_system_prompt()
USER_PROMPT_TEMPLATE = load_user_prompt_template()
NEWS_SEARCH_CONFIG = load_news_search_config()

GOOGLE_API_KEY_ENV_ORDER = [
    "GOOGLE_API_KEY",
    "GOOGLE_GENAI_API_KEY",
    "GENAI_API_KEY",
    "GEMINI_API_KEY",
]

DEFAULT_GEMINI_MODEL = (
    os.getenv("GOOGLE_GENAI_MODEL")
    or os.getenv("GEMINI_MODEL")
    or os.getenv("GEMINI_MODEL_NAME")
    or "gemini-2.5-flash-lite"
)

OPENAI_DEFAULT_MODEL = "gpt-4o-mini"


CHROME_PASSWORD_MANAGER_SCRIPT = """
<script>
(function attachChromePasswordHints() {
    const doc = window.parent?.document || window.document;
    const targets = [
        { selector: 'input[aria-label*="Gemini ãƒ¢ãƒ‡ãƒ«ID"]', name: 'gemini_model_id', autocomplete: 'username' },
        { selector: 'input[aria-label*="Google AI Studio API Key"]', name: 'google_ai_studio_api_key', autocomplete: 'current-password' },
        { selector: 'input[aria-label*="OpenAI API Key"]', name: 'openai_api_key', autocomplete: 'off' },
        { selector: 'input[aria-label*="ãƒ†ã‚£ãƒƒã‚«ãƒ¼ã‚·ãƒ³ãƒœãƒ«"]', name: 'ticker_symbol', autocomplete: 'off' },
    ];
    let attempts = 0;
    const maxAttempts = 20;
    const delay = 400;

    function applyAttributes() {
        let pending = false;
        targets.forEach((cfg) => {
            const input = doc.querySelector(cfg.selector);
            if (!input) {
                pending = true;
                return;
            }
            input.setAttribute('name', cfg.name);
            input.setAttribute('id', cfg.name);
            input.setAttribute('autocomplete', cfg.autocomplete);
            input.setAttribute('data-managed-by', 'chrome-password-manager');
        });
        if (pending && attempts < maxAttempts) {
            attempts += 1;
            setTimeout(applyAttributes, delay);
        }
    }

    if (doc.readyState === 'complete') {
        applyAttributes();
    } else {
        doc.addEventListener('readystatechange', () => {
            if (doc.readyState === 'complete') {
                applyAttributes();
            }
        });
    }
})();
</script>
"""


def enable_chrome_password_manager_support():
    """Inject autocomplete/name attributes so Chrome can store API keys & model IDs."""
    st.markdown(CHROME_PASSWORD_MANAGER_SCRIPT, unsafe_allow_html=True)


def build_api_status_snapshot(
    openai_key: str,
    google_key: str,
    model_name: str,
    applied_at: Optional[str] = None,
) -> Dict:
    """Summarize the current APIè¨­å®š status for UI display."""
    return {
        "openai_ready": bool((openai_key or "").strip()),
        "google_ready": bool((google_key or "").strip()),
        "model_name": (model_name or "").strip(),
        "last_applied": applied_at or "æœªé©ç”¨",
    }


def render_api_status_panel(status: Optional[Dict]):
    snapshot = status or {}
    openai_ready = snapshot.get("openai_ready", False)
    google_ready = snapshot.get("google_ready", False)
    model_name = snapshot.get("model_name") or "æœªæŒ‡å®š"
    last_applied = snapshot.get("last_applied") or "æœªé©ç”¨"

    st.markdown("### ğŸ”‘ APIè¨­å®šã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹")
    status_html = f"""
    <div class="api-status-panel">
        <div class="api-status-row">
            <span>OpenAI API Key</span>
            <span class="api-status-value {'active' if openai_ready else 'inactive'}">
                {'è¨­å®šæ¸ˆã¿' if openai_ready else 'æœªè¨­å®š'}
            </span>
        </div>
        <div class="api-status-row">
            <span>Google AI Studio API Key</span>
            <span class="api-status-value {'active' if google_ready else 'inactive'}">
                {'è¨­å®šæ¸ˆã¿' if google_ready else 'æœªè¨­å®š'}
            </span>
        </div>
        <div class="api-status-row">
            <span>Gemini ãƒ¢ãƒ‡ãƒ«ID</span>
            <span class="api-status-value">{model_name or 'æœªæŒ‡å®š'}</span>
        </div>
        <div class="api-status-row">
            <span>æœ€çµ‚é©ç”¨</span>
            <span class="api-status-value">{last_applied}</span>
        </div>
    </div>
    """
    st.markdown(status_html, unsafe_allow_html=True)


def build_ai_user_prompt(payload: Dict) -> str:
    """ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ§‹ç¯‰ã™ã‚‹"""
    market_data_json = json.dumps(payload, ensure_ascii=False)
    
    # ãƒ‹ãƒ¥ãƒ¼ã‚¹æ¤œç´¢çµæœã‚’ãƒ†ã‚­ã‚¹ãƒˆã«ã¾ã¨ã‚ã‚‹
    news_items = payload.get("news", [])
    news_text = ""
    if news_items:
        for n in news_items:
            title = n.get("title", "")
            snippet = n.get("snippet") or n.get("body", "")
            # ã‚¿ã‚¤ãƒˆãƒ«ã¨æœ¬æ–‡ï¼ˆsnippetï¼‰ã‚’çµåˆ
            news_text += f"- Title: {title}\n  Snippet: {snippet}\n"
    else:
        news_text = "ï¼ˆæœ€æ–°ãƒ‹ãƒ¥ãƒ¼ã‚¹æƒ…å ±ã¯å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸï¼‰"
    
    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã® {market_data} ã¨ {news_context} ã«æµã—è¾¼ã‚€
    return USER_PROMPT_TEMPLATE.format(
        market_data=market_data_json,
        news_context=news_text
    )


def resolve_google_api_key_from_env() -> str:
    for env_name in GOOGLE_API_KEY_ENV_ORDER:
        value = os.getenv(env_name)
        if value:
            return value
    return ""


def normalize_ticker_input(raw_symbol: str) -> Dict[str, str]:
    """Convert user input like '6501' to a resolvable yfinance symbol such as '6501.T'."""
    raw_symbol = (raw_symbol or "").strip()
    normalized = raw_symbol.upper().replace("ï¼´", "T").strip()
    normalized = re.sub(r"^(?:TYO|JPX|JP|TSE):", "", normalized)
    normalized = normalized.replace(" ", "")

    conversion_note = ""
    query_symbol = normalized
    display_symbol = normalized or raw_symbol

    if normalized.isdigit() and 4 <= len(normalized) <= 5:
        query_symbol = f"{normalized}.T"
        display_symbol = raw_symbol or query_symbol
        conversion_note = f"å›½å†…è¨¼åˆ¸ã‚³ãƒ¼ãƒ‰ {raw_symbol or normalized} ã‚’ {query_symbol} ã¨ã—ã¦å–å¾—ã—ã¾ã—ãŸã€‚"

    return {
        "input_symbol": raw_symbol,
        "query_symbol": query_symbol,
        "display_symbol": display_symbol or query_symbol,
        "conversion_note": conversion_note,
    }


def format_currency(value: Optional[float], currency: str = "USD") -> str:
    if value is None or (isinstance(value, float) and math.isnan(value)):
        return "â€”"
    currency = (currency or "USD").upper()
    symbol_map = {
        "USD": "$",
        "JPY": "Â¥",
        "EUR": "â‚¬",
    }
    symbol = symbol_map.get(currency, "")
    decimals = 0 if currency == "JPY" else 2
    return f"{symbol}{value:,.{decimals}f}"


def format_percent(value: Optional[float]) -> str:
    if value is None or (isinstance(value, float) and math.isnan(value)):
        return "â€”"
    return f"{value:.2f}%"


def safe_fast_info_get(fast_info, key: str):
    """yfinance fast_info sometimes raises KeyError when a field is missing."""
    if not fast_info:
        return None
    if isinstance(fast_info, dict):
        return fast_info.get(key)
    getter = getattr(fast_info, "get", None)
    if callable(getter):
        try:
            return getter(key)
        except KeyError:
            return None
        except Exception:
            pass
    try:
        return getattr(fast_info, key)
    except (AttributeError, KeyError):
        return None
    except Exception:
        return None


@st.cache_data(ttl=900, show_spinner=False)
def fetch_ticker_snapshot(symbol: str) -> Dict:
    symbol = symbol.upper().strip()
    if not symbol:
        return {"error": "ãƒ†ã‚£ãƒƒã‚«ãƒ¼ãŒæŒ‡å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚"}
    try:
        ticker = yf.Ticker(symbol)
        info = ticker.info or {}
        fast_info = getattr(ticker, "fast_info", {}) or {}
        hist = ticker.history(period="5d", interval="1d")
    except Exception as exc:  # pragma: no cover - network
        return {"error": f"ãƒ‡ãƒ¼ã‚¿å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ: {exc}"}

    price = safe_fast_info_get(fast_info, "last_price") or info.get("currentPrice")
    if price is None and not hist.empty:
        try:
            last_close = hist["Close"].iloc[-1]
            if not (isinstance(last_close, float) and math.isnan(last_close)):
                price = float(last_close)
        except (IndexError, KeyError, ValueError, TypeError):
            pass

    prev_close = safe_fast_info_get(fast_info, "previous_close") or info.get("previousClose")
    if prev_close is None and not hist.empty and len(hist) > 1:
        try:
            prev_close_value = hist["Close"].iloc[-2]
            if not (isinstance(prev_close_value, float) and math.isnan(prev_close_value)):
                prev_close = float(prev_close_value)
        except (IndexError, KeyError, ValueError, TypeError):
            pass

    day_change = day_change_pct = None
    if price is not None and prev_close is not None and prev_close != 0:
        day_change = price - prev_close
        day_change_pct = (day_change / prev_close) * 100

    currency = (
        safe_fast_info_get(fast_info, "currency")
        or info.get("currency")
        or info.get("financialCurrency")
        or "USD"
    )

    target_mean_price = info.get("targetMeanPrice")
    target_gap_pct = None
    if price is not None and price != 0 and target_mean_price is not None:
        target_gap_pct = ((target_mean_price - price) / price) * 100

    inst_pct = info.get("institutionPercent")
    # æ©Ÿé–¢æŠ•è³‡å®¶ä¿æœ‰æ¯”ç‡: 0-1ã®ç¯„å›²ã®å°æ•°ï¼ˆä¾‹ï¼š0.75 = 75%ï¼‰ã®å ´åˆã¯100ã‚’æ›ã‘ã¦ãƒ‘ãƒ¼ã‚»ãƒ³ãƒ†ãƒ¼ã‚¸ã«å¤‰æ›
    if inst_pct is not None:
        if 0 <= inst_pct <= 1:
            inst_pct = inst_pct * 100
        # æ—¢ã«ãƒ‘ãƒ¼ã‚»ãƒ³ãƒ†ãƒ¼ã‚¸å½¢å¼ï¼ˆ1ã‚ˆã‚Šå¤§ãã„ï¼‰ã®å ´åˆã¯ãã®ã¾ã¾ä½¿ç”¨
        elif inst_pct < 0:
            inst_pct = None  # è² ã®å€¤ã¯ç„¡åŠ¹

    # é…å½“åˆ©å›ã‚Š: yfinanceã¯æ—¢ã«ãƒ‘ãƒ¼ã‚»ãƒ³ãƒ†ãƒ¼ã‚¸å½¢å¼ã§è¿”ã™ï¼ˆä¾‹ï¼š0.95 = 0.95%ï¼‰
    dividend_yield_raw = info.get("dividendYield")
    dividend_yield_pct = None
    if dividend_yield_raw is not None:
        try:
            dividend_yield_float = float(dividend_yield_raw)
            # è² ã®å€¤ã¯ç„¡åŠ¹
            if dividend_yield_float < 0:
                dividend_yield_pct = None
            # 100ã‚’è¶…ãˆã‚‹å ´åˆã¯ç•°å¸¸å€¤ã®å¯èƒ½æ€§ãŒã‚ã‚‹ãŸã‚ã€ç„¡è¦–
            elif dividend_yield_float > 100:
                dividend_yield_pct = None  # ç•°å¸¸å€¤ã¨ã—ã¦ç„¡è¦–
            else:
                # ãã®ã¾ã¾ä½¿ç”¨ï¼ˆæ—¢ã«ãƒ‘ãƒ¼ã‚»ãƒ³ãƒ†ãƒ¼ã‚¸å½¢å¼ï¼‰
                dividend_yield_pct = dividend_yield_float
        except (ValueError, TypeError):
            dividend_yield_pct = None

    def safe_get_metric(key: str):
        """å®‰å…¨ã«æŒ‡æ¨™ã‚’å–å¾—ã—ã€NaNã‚„ç„¡åŠ¹ãªå€¤ã‚’Noneã«å¤‰æ›"""
        value = info.get(key)
        if value is None:
            return None
        if isinstance(value, float) and math.isnan(value):
            return None
        if isinstance(value, (int, float)) and math.isinf(value):
            # ç„¡é™å¤§ã¯ç„¡åŠ¹
            return None
        # è² ã®å€¤ãŒæœ‰åŠ¹ãªæŒ‡æ¨™ï¼ˆEPSã€Betaãªã©ï¼‰
        if key in ["trailingEps", "beta"]:
            return value
        # ãã®ä»–ã®æŒ‡æ¨™ã§è² ã®å€¤ã¯ç„¡åŠ¹
        if isinstance(value, (int, float)) and value < 0:
            return None
        return value

    key_metrics = {
        "trailingPE": safe_get_metric("trailingPE"),
        "forwardPE": safe_get_metric("forwardPE"),
        "pegRatio": safe_get_metric("pegRatio"),
        "priceToBook": safe_get_metric("priceToBook"),
        "trailingEps": safe_get_metric("trailingEps"),  # EPSã¯è² ã®å€¤ã‚‚æœ‰åŠ¹
        "dividendYield": dividend_yield_pct,
        "beta": safe_get_metric("beta"),
        "marketCap": safe_get_metric("marketCap"),
    }

    analyst_snapshot = {
        "recommendation_key": info.get("recommendationKey"),
        "recommendation_mean": info.get("recommendationMean"),
        "opinion_count": info.get("numberOfAnalystOpinions"),
        "target_mean_price": target_mean_price,
        "target_gap_pct": target_gap_pct,
        "institutional_ownership_pct": inst_pct,
    }

    ts = safe_fast_info_get(fast_info, "last_price_time")
    if isinstance(ts, (int, float)):
        market_time = datetime.fromtimestamp(ts, tz=timezone.utc).isoformat()
    else:
        market_time = datetime.now(timezone.utc).isoformat()

    # æ—¥æœ¬èªåã‚’å–å¾—ï¼ˆæ—¥æœ¬æ ªã®å ´åˆï¼‰
    company_name = info.get("longName") or info.get("shortName") or symbol
    symbol_clean = symbol.replace(".T", "").strip()
    if symbol_clean.isdigit():
        # æ—¥æœ¬æ ªã®å ´åˆã€æ—¥æœ¬èªåã‚’å„ªå…ˆçš„ã«ä½¿ç”¨
        japanese_name = get_japanese_company_name_cached(symbol, info)
        if japanese_name:
            company_name = japanese_name

    return {
        "error": None,
        "symbol": symbol,
        "company_name": company_name,
        "price": price,
        "previous_close": prev_close,
        "day_change": day_change,
        "day_change_pct": day_change_pct,
        "currency": currency,
        "market_time": market_time,
        "info": info,
        "analyst": analyst_snapshot,
        "key_metrics": key_metrics,
    }


@st.cache_data(ttl=300, show_spinner=False)
def fetch_stock_history(symbol: str, period: str = "1mo") -> Optional[Dict]:
    """æ ªä¾¡ã®æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã™ã‚‹"""
    symbol = symbol.upper().strip()
    if not symbol:
        return {"error": "ãƒ†ã‚£ãƒƒã‚«ãƒ¼ãŒæŒ‡å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚"}
    
    try:
        ticker = yf.Ticker(symbol)
        # period: 1d, 5d, 1mo, 3mo, 6mo, 1y, 2y, 5y, 10y, ytd, max
        # interval: 1m, 2m, 5m, 15m, 30m, 60m, 90m, 1h, 1d, 5d, 1wk, 1mo, 3mo
        hist = ticker.history(period=period)
        
        if hist.empty:
            return {"error": "ãƒ‡ãƒ¼ã‚¿ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚"}
        
        # ãƒ‡ãƒ¼ã‚¿ã‚’è¾æ›¸å½¢å¼ã«å¤‰æ›
        data = {
            "dates": hist.index.tolist(),
            "open": hist["Open"].tolist(),
            "high": hist["High"].tolist(),
            "low": hist["Low"].tolist(),
            "close": hist["Close"].tolist(),
            "volume": hist["Volume"].tolist(),
        }
        
        return {"error": None, "data": data, "symbol": symbol}
    except Exception as exc:
        return {"error": f"ãƒ‡ãƒ¼ã‚¿å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ: {exc}"}


def create_stock_chart(history_data: Dict, symbol: str, currency: str = "USD") -> go.Figure:
    """æ ªä¾¡ã®æ™‚ç³»åˆ—ã‚°ãƒ©ãƒ•ã‚’ä½œæˆã™ã‚‹ï¼ˆPlotlyï¼‰"""
    from plotly.subplots import make_subplots
    
    if history_data.get("error") or not history_data.get("data"):
        fig = go.Figure()
        fig.add_annotation(
            text="ãƒ‡ãƒ¼ã‚¿ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ",
            xref="paper", yref="paper",
            x=0.5, y=0.5,
            showarrow=False
        )
        return fig
    
    data = history_data["data"]
    dates = data["dates"]
    closes = data["close"]
    volumes = data["volume"]
    
    # ã‚µãƒ–ãƒ—ãƒ­ãƒƒãƒˆã‚’ä½œæˆï¼ˆä¾¡æ ¼ãƒãƒ£ãƒ¼ãƒˆã¨å‡ºæ¥é«˜ãƒãƒ£ãƒ¼ãƒˆï¼‰
    fig = make_subplots(
        rows=2, cols=1,
        shared_xaxes=True,
        vertical_spacing=0.1,
        row_heights=[0.7, 0.3],
        subplot_titles=("æ ªä¾¡", "å‡ºæ¥é«˜"),
    )
    
    # ãƒ­ãƒ¼ã‚½ã‚¯è¶³
    fig.add_trace(
        go.Candlestick(
            x=dates,
            open=data["open"],
            high=data["high"],
            low=data["low"],
            close=data["close"],
            name="ä¾¡æ ¼",
            increasing_line_color="#10b981",
            decreasing_line_color="#f87171",
        ),
        row=1, col=1
    )
    
    # å‡ºæ¥é«˜
    colors = ["#10b981" if closes[i] >= data["open"][i] else "#f87171" 
              for i in range(len(dates))]
    fig.add_trace(
        go.Bar(
            x=dates,
            y=volumes,
            name="å‡ºæ¥é«˜",
            marker_color=colors,
            opacity=0.6,
        ),
        row=2, col=1
    )
    
    # ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆè¨­å®š
    currency_symbol = "Â¥" if currency == "JPY" else "$"
    fig.update_layout(
        title=f"{symbol} æ ªä¾¡ãƒãƒ£ãƒ¼ãƒˆ",
        xaxis_title="æ—¥ä»˜",
        yaxis_title=f"ä¾¡æ ¼ ({currency_symbol})",
        yaxis2_title="å‡ºæ¥é«˜",
        height=600,
        template="plotly_dark",
        hovermode="x unified",
        showlegend=False,
        xaxis_rangeslider_visible=False,
    )
    
    # ã‚°ãƒ©ãƒ•ã®èƒŒæ™¯è‰²ã‚’ãƒ€ãƒ¼ã‚¯ãƒ†ãƒ¼ãƒã«åˆã‚ã›ã‚‹
    fig.update_layout(
        plot_bgcolor="#0f1116",
        paper_bgcolor="#0f1116",
        font_color="#f3f4f6",
    )
    
    return fig


def get_yahoo_finance_url(symbol: str) -> str:
    """Yahoo Financeã®URLã‚’ç”Ÿæˆ"""
    symbol_clean = symbol.replace(".T", "")
    if symbol_clean.isdigit():
        # æ—¥æœ¬æ ªã®å ´åˆ
        return f"https://finance.yahoo.co.jp/quote/{symbol_clean}.T"
    else:
        # æµ·å¤–æ ªã®å ´åˆ
        return f"https://finance.yahoo.com/quote/{symbol}"


def parse_news_date(date_str: Optional[str]) -> Optional[datetime]:
    """ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®æ—¥ä»˜æ–‡å­—åˆ—ã‚’ãƒ‘ãƒ¼ã‚¹ã—ã¦datetimeã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«å¤‰æ›"""
    if not date_str:
        return None
    
    # æ§˜ã€…ãªæ—¥ä»˜å½¢å¼ã«å¯¾å¿œ
    date_formats = [
        "%Y-%m-%dT%H:%M:%S%z",  # ISOå½¢å¼ï¼ˆã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³ä»˜ãï¼‰
        "%Y-%m-%dT%H:%M:%S",     # ISOå½¢å¼ï¼ˆã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³ãªã—ï¼‰
        "%Y-%m-%d %H:%M:%S",     # æ¨™æº–å½¢å¼
        "%Y-%m-%d",              # æ—¥ä»˜ã®ã¿
        "%d %b %Y",              # "01 Jan 2024"
        "%d %B %Y",              # "01 January 2024"
        "%Yå¹´%mæœˆ%dæ—¥",           # æ—¥æœ¬èªå½¢å¼
    ]
    
    for fmt in date_formats:
        try:
            return datetime.strptime(date_str, fmt)
        except (ValueError, TypeError):
            continue
    
    # ç›¸å¯¾çš„ãªæ—¥ä»˜è¡¨ç¾ï¼ˆä¾‹ï¼š"2æ™‚é–“å‰"ã€"3æ—¥å‰"ï¼‰ã®å‡¦ç†
    if isinstance(date_str, str):
        date_str_lower = date_str.lower()
        now = datetime.now(timezone.utc)
        
        # "æ™‚é–“å‰"ã€"æ—¥å‰"ãªã©ã®ç›¸å¯¾è¡¨ç¾ã‚’å‡¦ç†
        import re
        hours_match = re.search(r'(\d+)\s*æ™‚é–“å‰', date_str_lower)
        if hours_match:
            hours = int(hours_match.group(1))
            return now - timedelta(hours=hours)
        
        days_match = re.search(r'(\d+)\s*æ—¥å‰', date_str_lower)
        if days_match:
            days = int(days_match.group(1))
            return now - timedelta(days=days)
    
    return None


def filter_recent_news(news_items: List[Dict], days_threshold: int = 30) -> List[Dict]:
    """æŒ‡å®šæ—¥æ•°ä»¥å†…ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®ã¿ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°"""
    if not news_items:
        return []
    
    threshold_date = datetime.now(timezone.utc) - timedelta(days=days_threshold)
    filtered = []
    
    for item in news_items:
        date_str = item.get("published")
        if not date_str:
            # æ—¥ä»˜æƒ…å ±ãŒãªã„å ´åˆã¯å«ã‚ã‚‹ï¼ˆæœ€æ–°ã®å¯èƒ½æ€§ãŒã‚ã‚‹ï¼‰
            filtered.append(item)
            continue
        
        parsed_date = parse_news_date(date_str)
        if parsed_date:
            # ã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³æƒ…å ±ãŒãªã„å ´åˆã¯UTCã¨ä»®å®š
            if parsed_date.tzinfo is None:
                parsed_date = parsed_date.replace(tzinfo=timezone.utc)
            
            if parsed_date >= threshold_date:
                filtered.append(item)
        else:
            # ãƒ‘ãƒ¼ã‚¹ã§ããªã„å ´åˆã¯å«ã‚ã‚‹ï¼ˆæœ€æ–°ã®å¯èƒ½æ€§ãŒã‚ã‚‹ï¼‰
            filtered.append(item)
    
    return filtered


def is_shallow_article(item: Dict, company_name: Optional[str] = None, symbol: Optional[str] = None) -> bool:
    """ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã‚„å¸‚å ´å‹•å‘ã®ã‚ˆã†ãªè–„ã„è¨˜äº‹ã‹ã‚’åˆ¤å®š"""
    config = NEWS_SEARCH_CONFIG.get("keywords_for_scoring", {}).get("shallow_article", {})
    shallow_keywords_ja = config.get("japanese", [])
    shallow_keywords_en = config.get("english", [])
    
    filtering_config = NEWS_SEARCH_CONFIG.get("filtering", {}).get("shallow_article", {})
    min_stock_codes = filtering_config.get("min_stock_codes", 3)
    
    title = (item.get("title") or "").lower()
    snippet = (item.get("snippet") or "").lower()
    text = f"{title} {snippet}"
    
    # ã‚¿ã‚¤ãƒˆãƒ«ã«ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã‚„å¸‚å ´å‹•å‘ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹
    for keyword in shallow_keywords_ja + shallow_keywords_en:
        if keyword in text:
            # ãŸã ã—ã€å¯¾è±¡éŠ˜æŸ„åãŒã‚¿ã‚¤ãƒˆãƒ«ã«å«ã¾ã‚Œã¦ã„ã‚‹å ´åˆã¯é™¤å¤–ï¼ˆå¯¾è±¡éŠ˜æŸ„ã«ç„¦ç‚¹ã‚’å½“ã¦ãŸãƒ©ãƒ³ã‚­ãƒ³ã‚°è¨˜äº‹ã®å¯èƒ½æ€§ï¼‰
            if company_name and company_name.lower() in title:
                continue
            if symbol and symbol.replace(".T", "").strip().lower() in title:
                continue
            return True
    
    # è¤‡æ•°ã®éŠ˜æŸ„ã‚³ãƒ¼ãƒ‰ãŒå«ã¾ã‚Œã¦ã„ã‚‹å ´åˆï¼ˆè¨­å®šå€¤ä»¥ä¸Šï¼‰ã¯è–„ã„è¨˜äº‹ã®å¯èƒ½æ€§ãŒé«˜ã„
    if symbol:
        symbol_clean = symbol.replace(".T", "").strip()
        if symbol_clean.isdigit():
            # 4æ¡ã®æ•°å­—ï¼ˆéŠ˜æŸ„ã‚³ãƒ¼ãƒ‰ï¼‰ãŒè¨­å®šå€¤ä»¥ä¸Šå«ã¾ã‚Œã¦ã„ã‚‹ã‹
            stock_codes = re.findall(r'\b\d{4}\b', text)
            if len(stock_codes) >= min_stock_codes:
                # å¯¾è±¡éŠ˜æŸ„ãŒå«ã¾ã‚Œã¦ã„ã¦ã‚‚ã€ä»–ã®éŠ˜æŸ„ãŒå¤šãå«ã¾ã‚Œã¦ã„ã‚‹å ´åˆã¯è–„ã„è¨˜äº‹
                if symbol_clean not in stock_codes:
                    return True
                # å¯¾è±¡éŠ˜æŸ„ãŒå«ã¾ã‚Œã¦ã„ã¦ã‚‚ã€è¨­å®šå€¤ä»¥ä¸Šã®éŠ˜æŸ„ãŒå«ã¾ã‚Œã¦ã„ã‚‹å ´åˆã¯å¸‚å ´å‹•å‘è¨˜äº‹ã®å¯èƒ½æ€§ãŒé«˜ã„
                if len(set(stock_codes)) >= min_stock_codes:
                    return True
    
    return False


def calculate_focus_score(item: Dict, company_name: Optional[str] = None, symbol: Optional[str] = None, query: Optional[str] = None) -> int:
    """å¯¾è±¡éŠ˜æŸ„ã¸ã®ç„¦ç‚¹åº¦ã‚’ã‚¹ã‚³ã‚¢åŒ–ï¼ˆé«˜ã„ã»ã©å¯¾è±¡éŠ˜æŸ„ã«ç„¦ç‚¹ã‚’å½“ã¦ã¦ã„ã‚‹ï¼‰"""
    scoring_config = NEWS_SEARCH_CONFIG.get("scoring", {}).get("focus_score", {})
    keywords_config = NEWS_SEARCH_CONFIG.get("keywords_for_scoring", {}).get("deep_analysis", {})
    deep_analysis_keywords_ja = keywords_config.get("japanese", [])
    deep_analysis_keywords_en = keywords_config.get("english", [])
    
    title = (item.get("title") or "").lower()
    snippet = (item.get("snippet") or "").lower()
    text = f"{title} {snippet}"
    
    score = 0
    
    # å¯¾è±¡éŠ˜æŸ„åãŒã‚¿ã‚¤ãƒˆãƒ«ã«å«ã¾ã‚Œã¦ã„ã‚‹å ´åˆã¯é«˜ã‚¹ã‚³ã‚¢
    if company_name:
        company_name_lower = company_name.lower()
        if company_name_lower in title:
            score += scoring_config.get("company_name_in_title", 10)
        if company_name_lower in snippet:
            score += scoring_config.get("company_name_in_snippet", 5)
        
        # å¯¾è±¡éŠ˜æŸ„åã®å‡ºç¾å›æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
        count = text.count(company_name_lower)
        multiplier = scoring_config.get("company_name_count_multiplier", 2)
        max_score = scoring_config.get("company_name_count_max", 10)
        score += min(count * multiplier, max_score)
    
    # ãƒ†ã‚£ãƒƒã‚«ãƒ¼ã‚·ãƒ³ãƒœãƒ«ãŒå«ã¾ã‚Œã¦ã„ã‚‹å ´åˆã‚‚ã‚¹ã‚³ã‚¢åŠ ç®—
    if symbol:
        symbol_clean = symbol.replace(".T", "").strip().lower()
        if symbol_clean in title:
            score += scoring_config.get("symbol_in_title", 8)
        if symbol_clean in snippet:
            score += scoring_config.get("symbol_in_snippet", 4)
        
        # ãƒ†ã‚£ãƒƒã‚«ãƒ¼ã‚·ãƒ³ãƒœãƒ«ã®å‡ºç¾å›æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
        count = text.count(symbol_clean)
        multiplier = scoring_config.get("symbol_count_multiplier", 2)
        max_score = scoring_config.get("symbol_count_max", 8)
        score += min(count * multiplier, max_score)
    
    # ã‚¯ã‚¨ãƒªï¼ˆè‹±èªã®ç¤¾åãªã©ï¼‰ãŒå«ã¾ã‚Œã¦ã„ã‚‹å ´åˆã‚‚ã‚¹ã‚³ã‚¢åŠ ç®—
    if query:
        query_lower = query.lower()
        if query_lower in title:
            score += scoring_config.get("query_in_title", 6)
        if query_lower in snippet:
            score += scoring_config.get("query_in_snippet", 3)
    
    # æ·±ã„åˆ†æã‚’ç¤ºã™ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒå«ã¾ã‚Œã¦ã„ã‚‹å ´åˆã¯ãƒœãƒ¼ãƒŠã‚¹
    bonus = scoring_config.get("deep_analysis_bonus", 2)
    for keyword in deep_analysis_keywords_ja + deep_analysis_keywords_en:
        if keyword in text:
            score += bonus
    
    return score


def calculate_news_importance_score(item: Dict) -> int:
    """ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®é‡è¦åº¦ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—ï¼ˆé‡è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ï¼‰"""
    keywords_config = NEWS_SEARCH_CONFIG.get("keywords_for_scoring", {}).get("important", {})
    important_keywords_ja = keywords_config.get("japanese", [])
    important_keywords_en = keywords_config.get("english", [])
    
    scoring_config = NEWS_SEARCH_CONFIG.get("scoring", {}).get("importance_score", {})
    keyword_score = scoring_config.get("keyword_score", 2)
    
    title = (item.get("title") or "").lower()
    snippet = (item.get("snippet") or "").lower()
    text = f"{title} {snippet}"
    
    score = 0
    for keyword in important_keywords_ja + important_keywords_en:
        if keyword in text:
            score += keyword_score  # é‡è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒè¦‹ã¤ã‹ã£ãŸã‚‰ã‚¹ã‚³ã‚¢ã‚’åŠ ç®—
    
    return score


def sort_news_by_importance_and_date(news_items: List[Dict], reverse: bool = True, company_name: Optional[str] = None, symbol: Optional[str] = None, query: Optional[str] = None) -> List[Dict]:
    """ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’é‡è¦åº¦ã€ç„¦ç‚¹åº¦ã€æ—¥ä»˜ã§ã‚½ãƒ¼ãƒˆï¼ˆé‡è¦åº¦ã¨ç„¦ç‚¹åº¦ãŒé«˜ã„é †ã€åŒã˜ãªã‚‰æ–°ã—ã„é †ï¼‰"""
    def get_sort_key(item: Dict) -> tuple:
        # é‡è¦åº¦ã‚¹ã‚³ã‚¢ï¼ˆé«˜ã„æ–¹ãŒå„ªå…ˆï¼‰
        importance_score = calculate_news_importance_score(item)
        
        # ç„¦ç‚¹åº¦ã‚¹ã‚³ã‚¢ï¼ˆå¯¾è±¡éŠ˜æŸ„ã«ç„¦ç‚¹ã‚’å½“ã¦ã¦ã„ã‚‹ã»ã©é«˜ã„ï¼‰
        focus_score = calculate_focus_score(item, company_name, symbol, query)
        
        # æ—¥ä»˜
        date_str = item.get("published")
        if not date_str:
            # æ—¥ä»˜ãŒãªã„å ´åˆã¯æœ€ã‚‚å¤ã„æ—¥ä»˜ã¨ã—ã¦æ‰±ã†
            parsed_date = datetime(1970, 1, 1, tzinfo=timezone.utc)
        else:
            parsed_date = parse_news_date(date_str)
            if parsed_date:
                if parsed_date.tzinfo is None:
                    parsed_date = parsed_date.replace(tzinfo=timezone.utc)
            else:
                parsed_date = datetime(1970, 1, 1, tzinfo=timezone.utc)
        
        # é‡è¦åº¦ã‚¹ã‚³ã‚¢ï¼ˆé™é †ï¼‰ã€ç„¦ç‚¹åº¦ã‚¹ã‚³ã‚¢ï¼ˆé™é †ï¼‰ã€æ—¥ä»˜ï¼ˆé™é †ï¼‰ã§ã‚½ãƒ¼ãƒˆ
        # reverse=Trueã®å ´åˆã€(-importance_score, -focus_score, -parsed_date.timestamp()) ã§ã‚½ãƒ¼ãƒˆ
        # reverse=Falseã®å ´åˆã€ãã®é€†
        if reverse:
            return (-importance_score, -focus_score, -parsed_date.timestamp())
        else:
            return (importance_score, focus_score, parsed_date.timestamp())
    
    return sorted(news_items, key=get_sort_key)


def sort_news_by_date(news_items: List[Dict], reverse: bool = True) -> List[Dict]:
    """ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’æ—¥ä»˜ã§ã‚½ãƒ¼ãƒˆï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯æ–°ã—ã„é †ï¼‰"""
    def get_sort_key(item: Dict) -> datetime:
        date_str = item.get("published")
        if not date_str:
            # æ—¥ä»˜ãŒãªã„å ´åˆã¯æœ€ã‚‚å¤ã„æ—¥ä»˜ã¨ã—ã¦æ‰±ã†
            return datetime(1970, 1, 1, tzinfo=timezone.utc)
        
        parsed_date = parse_news_date(date_str)
        if parsed_date:
            if parsed_date.tzinfo is None:
                parsed_date = parsed_date.replace(tzinfo=timezone.utc)
            return parsed_date
        
        return datetime(1970, 1, 1, tzinfo=timezone.utc)
    
    return sorted(news_items, key=get_sort_key, reverse=reverse)


def is_japanese_text(text: str) -> bool:
    """ãƒ†ã‚­ã‚¹ãƒˆãŒæ—¥æœ¬èªã‚’å«ã‚€ã‹ã©ã†ã‹ã‚’åˆ¤å®š"""
    if not text:
        return False
    # ã²ã‚‰ãŒãªã€ã‚«ã‚¿ã‚«ãƒŠã€æ¼¢å­—ã€å…¨è§’è‹±æ•°å­—ã®ç¯„å›²ã‚’ãƒã‚§ãƒƒã‚¯
    japanese_pattern = re.compile(r'[\u3040-\u309F\u30A0-\u30FF\u4E00-\u9FAF\uFF00-\uFFEF]')
    return bool(japanese_pattern.search(text))


def get_japanese_name_from_yfinance(symbol: str) -> Optional[str]:
    """yfinanceã®infoã‹ã‚‰æ—¥æœ¬èªåã‚’å–å¾—"""
    try:
        ticker = yf.Ticker(symbol)
        info = ticker.info or {}
        
        # longNameã¾ãŸã¯shortNameãŒæ—¥æœ¬èªã®å ´åˆã€ãã‚Œã‚’è¿”ã™
        for key in ["longName", "shortName", "name"]:
            value = info.get(key)
            if value and isinstance(value, str) and is_japanese_text(value):
                return value.strip()
    except Exception as e:
        logging.debug(f"yfinanceã‹ã‚‰æ—¥æœ¬èªåå–å¾—å¤±æ•— ({symbol}): {e}")
    return None


def get_japanese_name_from_yahoo_finance_jp(symbol: str) -> Optional[str]:
    """Yahoo Finance Japanã‹ã‚‰æ—¥æœ¬èªåã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°"""
    if not SCRAPING_AVAILABLE:
        return None
    
    symbol_clean = symbol.replace(".T", "").strip()
    if not symbol_clean.isdigit():
        return None
    
    try:
        # Yahoo Finance Japanã®URL
        url = f"https://finance.yahoo.co.jp/quote/{symbol_clean}.T"
        
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
        }
        
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.content, "lxml")
        
        # è¤‡æ•°ã®ã‚»ãƒ¬ã‚¯ã‚¿ã‚’è©¦è¡Œ
        selectors = [
            'h1[data-test="company-name"]',
            'h1.company-name',
            'h1',
            '[data-test="company-name"]',
            '.company-name',
        ]
        
        for selector in selectors:
            element = soup.select_one(selector)
            if element:
                text = element.get_text(strip=True)
                if text and is_japanese_text(text):
                    return text
        
        # titleã‚¿ã‚°ã‹ã‚‰å–å¾—ã‚’è©¦è¡Œ
        title = soup.find("title")
        if title:
            title_text = title.get_text(strip=True)
            # ã‚¿ã‚¤ãƒˆãƒ«ã‹ã‚‰ã€Œ(6501.T)ã€ã®ã‚ˆã†ãªéƒ¨åˆ†ã‚’é™¤å»
            title_text = re.sub(r'\s*\([0-9]+\.T\)\s*', '', title_text)
            if title_text and is_japanese_text(title_text):
                return title_text
                
    except Exception as e:
        logging.debug(f"Yahoo Finance Japanã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å¤±æ•— ({symbol}): {e}")
    
    return None


def get_japanese_company_name(symbol: str, yfinance_info: Optional[Dict] = None) -> Optional[str]:
    """ãƒ†ã‚£ãƒƒã‚«ãƒ¼ã‚·ãƒ³ãƒœãƒ«ã‹ã‚‰æ—¥æœ¬èªã®ç¤¾åã‚’å–å¾—ã™ã‚‹ï¼ˆè¤‡æ•°ã®æ–¹æ³•ã‚’è©¦è¡Œï¼‰"""
    if not symbol:
        return None
    
    symbol_clean = symbol.replace(".T", "").strip()
    if not symbol_clean.isdigit():
        return None
    
    # 1. yfinanceã®infoã‹ã‚‰å–å¾—ï¼ˆå¼•æ•°ã§æ¸¡ã•ã‚ŒãŸå ´åˆï¼‰
    if yfinance_info:
        for key in ["longName", "shortName", "name"]:
            value = yfinance_info.get(key)
            if value and isinstance(value, str) and is_japanese_text(value):
                return value.strip()
    
    # 2. yfinanceã®APIã‹ã‚‰ç›´æ¥å–å¾—
    japanese_name = get_japanese_name_from_yfinance(symbol)
    if japanese_name:
        return japanese_name
    
    # 3. Yahoo Finance Japanã‹ã‚‰ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
    japanese_name = get_japanese_name_from_yahoo_finance_jp(symbol)
    if japanese_name:
        return japanese_name
    
    # 4. ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ä¸»è¦ãªæ—¥æœ¬æ ªã®ãƒ†ã‚£ãƒƒã‚«ãƒ¼ã‚·ãƒ³ãƒœãƒ«ã¨æ—¥æœ¬èªç¤¾åã®ãƒãƒƒãƒ”ãƒ³ã‚°
    japanese_names = {
        "6501": "æ—¥ç«‹è£½ä½œæ‰€",
        "6502": "æ±èŠ",
        "6503": "ä¸‰è±é›»æ©Ÿ",
        "6758": "ã‚½ãƒ‹ãƒ¼ã‚°ãƒ«ãƒ¼ãƒ—",
        "7203": "ãƒˆãƒ¨ã‚¿è‡ªå‹•è»Š",
        "6752": "ãƒ‘ãƒŠã‚½ãƒ‹ãƒƒã‚¯",
        "9984": "ã‚½ãƒ•ãƒˆãƒãƒ³ã‚¯ã‚°ãƒ«ãƒ¼ãƒ—",
        "9434": "ã‚½ãƒ•ãƒˆãƒãƒ³ã‚¯",
        "9983": "ãƒ•ã‚¡ãƒ¼ã‚¹ãƒˆãƒªãƒ†ã‚¤ãƒªãƒ³ã‚°",
        "8031": "ä¸‰äº•ç‰©ç”£",
        "8058": "ä¸‰è±å•†äº‹",
        "8001": "ä¼Šè—¤å¿ å•†äº‹",
        "8002": "ä¸¸ç´…",
        "8306": "ä¸‰è±UFJãƒ•ã‚£ãƒŠãƒ³ã‚·ãƒ£ãƒ«ãƒ»ã‚°ãƒ«ãƒ¼ãƒ—",
        "8316": "ä¸‰äº•ä½å‹ãƒ•ã‚£ãƒŠãƒ³ã‚·ãƒ£ãƒ«ã‚°ãƒ«ãƒ¼ãƒ—",
        "8411": "ã¿ãšã»ãƒ•ã‚£ãƒŠãƒ³ã‚·ãƒ£ãƒ«ã‚°ãƒ«ãƒ¼ãƒ—",
        "4063": "ä¿¡è¶ŠåŒ–å­¦å·¥æ¥­",
        "4061": "ãƒ‡ãƒ³ã‚«",
        "3401": "å¸äºº",
        "3402": "æ±ãƒ¬",
        "3405": "ã‚¯ãƒ©ãƒ¬",
        "3407": "æ—­åŒ–æˆ",
        "4911": "è³‡ç”Ÿå ‚",
        "4912": "ãƒ©ã‚¤ã‚ªãƒ³",
        "4452": "èŠ±ç‹",
        "4453": "è³‡ç”Ÿå ‚",
        "6098": "ãƒªã‚¯ãƒ«ãƒ¼ãƒˆãƒ›ãƒ¼ãƒ«ãƒ‡ã‚£ãƒ³ã‚°ã‚¹",
        "6099": "ã‚¨ãƒ«ãƒ”ãƒ¼ãƒ€ãƒ¡ãƒ¢ãƒª",
        "6178": "æ—¥æœ¬éƒµæ”¿",
        "6179": "æ—¥æœ¬éƒµæ”¿",
        "8801": "ä¸‰äº•ä¸å‹•ç”£",
        "8802": "ä¸‰è±åœ°æ‰€",
        "2914": "æ—¥æœ¬ãŸã°ã“ç”£æ¥­",
        "2501": "ã‚µãƒƒãƒãƒ­ãƒ›ãƒ¼ãƒ«ãƒ‡ã‚£ãƒ³ã‚°ã‚¹",
        "2502": "ã‚¢ã‚µãƒ’ã‚°ãƒ«ãƒ¼ãƒ—ãƒ›ãƒ¼ãƒ«ãƒ‡ã‚£ãƒ³ã‚°ã‚¹",
        "2503": "ã‚­ãƒªãƒ³ãƒ›ãƒ¼ãƒ«ãƒ‡ã‚£ãƒ³ã‚°ã‚¹",
        "2531": "å®ãƒ›ãƒ¼ãƒ«ãƒ‡ã‚£ãƒ³ã‚°ã‚¹",
        "2801": "ã‚­ãƒƒã‚³ãƒ¼ãƒãƒ³",
        "2802": "å‘³ã®ç´ ",
        "2871": "ãƒ‹ãƒãƒ¬ã‚¤",
        "3101": "æ±æ´‹ç´¡",
        "3103": "ãƒ¦ãƒ‹ãƒ»ãƒãƒ£ãƒ¼ãƒ ",
        "3105": "æ—¥æ¸…ç´¡ãƒ›ãƒ¼ãƒ«ãƒ‡ã‚£ãƒ³ã‚°ã‚¹",
        "3401": "å¸äºº",
        "3402": "æ±ãƒ¬",
        "3405": "ã‚¯ãƒ©ãƒ¬",
        "3407": "æ—­åŒ–æˆ",
        "4005": "ä½å‹åŒ–å­¦",
        "4004": "æ˜­å’Œé›»å·¥",
        "4003": "ã‚³ã‚¹ãƒ¢ã‚¨ãƒãƒ«ã‚®ãƒ¼ãƒ›ãƒ¼ãƒ«ãƒ‡ã‚£ãƒ³ã‚°ã‚¹",
        "4061": "ãƒ‡ãƒ³ã‚«",
        "4063": "ä¿¡è¶ŠåŒ–å­¦å·¥æ¥­",
        "4183": "ä¸‰äº•åŒ–å­¦",
        "4188": "ä¸‰è±ã‚±ãƒŸã‚«ãƒ«ãƒ›ãƒ¼ãƒ«ãƒ‡ã‚£ãƒ³ã‚°ã‚¹",
        "4208": "å®‡éƒ¨èˆˆç”£",
        "4272": "æ—¥æœ¬åŒ–è–¬",
        "4452": "èŠ±ç‹",
        "4453": "è³‡ç”Ÿå ‚",
        "4502": "æ­¦ç”°è–¬å“å·¥æ¥­",
        "4503": "ã‚¢ã‚¹ãƒ†ãƒ©ã‚¹è£½è–¬",
        "4506": "å¤§æ—¥æœ¬ä½å‹è£½è–¬",
        "4507": "å¡©é‡ç¾©è£½è–¬",
        "4519": "ä¸­å¤–è£½è–¬",
        "4523": "ã‚¨ãƒ¼ã‚¶ã‚¤",
        "4527": "ãƒ­ãƒ¼ãƒˆè£½è–¬",
        "4528": "å°é‡è–¬å“å·¥æ¥­",
        "4543": "ãƒ†ãƒ«ãƒ¢",
        "4568": "ç¬¬ä¸€ä¸‰å…±",
        "4578": "å¤§å¡šãƒ›ãƒ¼ãƒ«ãƒ‡ã‚£ãƒ³ã‚°ã‚¹",
        "4612": "æ—¥æœ¬ãƒšã‚¤ãƒ³ãƒˆãƒ›ãƒ¼ãƒ«ãƒ‡ã‚£ãƒ³ã‚°ã‚¹",
        "4661": "ã‚ªãƒªã‚¨ãƒ³ã‚¿ãƒ«ãƒ©ãƒ³ãƒ‰",
        "4684": "ã‚ªãƒ ãƒ­ãƒ³",
        "4689": "ãƒ¤ãƒ•ãƒ¼",
        "4704": "ãƒˆãƒ¬ãƒ³ãƒ‰ãƒã‚¤ã‚¯ãƒ­",
        "4751": "ã‚µã‚¤ãƒãƒ¼ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ",
        "4755": "æ¥½å¤©ã‚°ãƒ«ãƒ¼ãƒ—",
        "4901": "å¯Œå£«ãƒ•ã‚¤ãƒ«ãƒ ãƒ›ãƒ¼ãƒ«ãƒ‡ã‚£ãƒ³ã‚°ã‚¹",
        "4911": "è³‡ç”Ÿå ‚",
        "5019": "å‡ºå…‰èˆˆç”£",
        "5020": "ENEOSãƒ›ãƒ¼ãƒ«ãƒ‡ã‚£ãƒ³ã‚°ã‚¹",
        "5101": "æ¨ªæµœã‚´ãƒ ",
        "5108": "ãƒ–ãƒªãƒ‚ã‚¹ãƒˆãƒ³",
        "5201": "AGC",
        "5214": "æ—¥æœ¬é›»æ°—ç¡å­",
        "5232": "ä½å‹å¤§é˜ªã‚»ãƒ¡ãƒ³ãƒˆ",
        "5233": "å¤ªå¹³æ´‹ã‚»ãƒ¡ãƒ³ãƒˆ",
        "5301": "æ±æµ·ã‚«ãƒ¼ãƒœãƒ³",
        "5332": "TOTO",
        "5333": "æ—¥æœ¬ã‚¬ã‚¤ã‚·",
        "5401": "æ—¥æœ¬è£½é‰„",
        "5406": "ç¥æˆ¸è£½é‹¼æ‰€",
        "5411": "JFEãƒ›ãƒ¼ãƒ«ãƒ‡ã‚£ãƒ³ã‚°ã‚¹",
        "5541": "å¤§å¹³æ´‹é‡‘å±",
        "5631": "æ—¥æœ¬è£½é‹¼æ‰€",
        "5703": "æ—¥æœ¬è»½é‡‘å±ãƒ›ãƒ¼ãƒ«ãƒ‡ã‚£ãƒ³ã‚°ã‚¹",
        "5711": "ä¸‰è±ãƒãƒ†ãƒªã‚¢ãƒ«",
        "5713": "ä½å‹é‡‘å±é‰±å±±",
        "5714": "DOWAãƒ›ãƒ¼ãƒ«ãƒ‡ã‚£ãƒ³ã‚°ã‚¹",
        "5801": "å¤æ²³é›»æ°—å·¥æ¥­",
        "5802": "ä½å‹é›»æ°—å·¥æ¥­",
        "5803": "ãƒ•ã‚¸ã‚¯ãƒ©",
        "6098": "ãƒªã‚¯ãƒ«ãƒ¼ãƒˆãƒ›ãƒ¼ãƒ«ãƒ‡ã‚£ãƒ³ã‚°ã‚¹",
        "6178": "æ—¥æœ¬éƒµæ”¿",
        "6301": "ã‚³ãƒãƒ„",
        "6302": "ä½å‹é‡æ©Ÿæ¢°å·¥æ¥­",
        "6305": "æ—¥ç«‹å»ºæ©Ÿ",
        "6326": "ã‚¯ãƒœã‚¿",
        "6361": "èåŸè£½ä½œæ‰€",
        "6367": "ãƒ€ã‚¤ã‚­ãƒ³å·¥æ¥­",
        "6471": "æ—¥æœ¬ç²¾å·¥",
        "6472": "NTN",
        "6473": "ã‚¸ã‚§ã‚¤ãƒ†ã‚¯ãƒˆ",
        "6501": "æ—¥ç«‹è£½ä½œæ‰€",
        "6502": "æ±èŠ",
        "6503": "ä¸‰è±é›»æ©Ÿ",
        "6504": "å¯Œå£«é›»æ©Ÿ",
        "6506": "å®‰å·é›»æ©Ÿ",
        "6594": "æ—¥æœ¬é›»ç”£",
        "6701": "æ—¥æœ¬é›»æ°—",
        "6702": "å¯Œå£«é€š",
        "6723": "ãƒ«ãƒã‚µã‚¹ã‚¨ãƒ¬ã‚¯ãƒˆãƒ­ãƒ‹ã‚¯ã‚¹",
        "6724": "ã‚»ã‚¤ã‚³ãƒ¼ã‚¨ãƒ—ã‚½ãƒ³",
        "6752": "ãƒ‘ãƒŠã‚½ãƒ‹ãƒƒã‚¯",
        "6758": "ã‚½ãƒ‹ãƒ¼ã‚°ãƒ«ãƒ¼ãƒ—",
        "6770": "ã‚¢ãƒ«ãƒ—ã‚¹ã‚¢ãƒ«ãƒ‘ã‚¤ãƒ³",
        "6841": "æ¨ªæ²³é›»æ©Ÿ",
        "6857": "ã‚¢ãƒ‰ãƒãƒ³ãƒ†ã‚¹ãƒˆ",
        "6861": "ã‚­ãƒ¼ã‚¨ãƒ³ã‚¹",
        "6902": "ãƒ‡ãƒ³ã‚½ãƒ¼",
        "6954": "ãƒ•ã‚¡ãƒŠãƒƒã‚¯",
        "6971": "äº¬ã‚»ãƒ©",
        "6976": "å¤ªé™½èª˜é›»",
        "6981": "æ‘ç”°è£½ä½œæ‰€",
        "7011": "ä¸‰è±é‡å·¥æ¥­",
        "7012": "å·å´é‡å·¥æ¥­",
        "7013": "IHI",
        "7201": "æ—¥ç”£è‡ªå‹•è»Š",
        "7202": "ã„ã™ã‚è‡ªå‹•è»Š",
        "7203": "ãƒˆãƒ¨ã‚¿è‡ªå‹•è»Š",
        "7205": "æ—¥é‡è‡ªå‹•è»Š",
        "7261": "ãƒãƒ„ãƒ€",
        "7267": "ãƒ›ãƒ³ãƒ€",
        "7269": "ã‚¹ã‚ºã‚­",
        "7270": "SUBARU",
        "7272": "ãƒ¤ãƒãƒç™ºå‹•æ©Ÿ",
        "7731": "ãƒ‹ã‚³ãƒ³",
        "7732": "ãƒˆãƒ—ã‚³ãƒ³",
        "7733": "ã‚ªãƒªãƒ³ãƒ‘ã‚¹",
        "7741": "HOYA",
        "7751": "ã‚­ãƒ¤ãƒãƒ³",
        "7832": "ãƒãƒ³ãƒ€ã‚¤ãƒŠãƒ ã‚³ãƒ›ãƒ¼ãƒ«ãƒ‡ã‚£ãƒ³ã‚°ã‚¹",
        "7911": "å‡¸ç‰ˆå°åˆ·",
        "7912": "å¤§æ—¥æœ¬å°åˆ·",
        "8001": "ä¼Šè—¤å¿ å•†äº‹",
        "8002": "ä¸¸ç´…",
        "8015": "è±Šç”°é€šå•†",
        "8031": "ä¸‰äº•ç‰©ç”£",
        "8058": "ä¸‰è±å•†äº‹",
        "8060": "é‡æ‘ãƒ›ãƒ¼ãƒ«ãƒ‡ã‚£ãƒ³ã‚°ã‚¹",
        "8306": "ä¸‰è±UFJãƒ•ã‚£ãƒŠãƒ³ã‚·ãƒ£ãƒ«ãƒ»ã‚°ãƒ«ãƒ¼ãƒ—",
        "8316": "ä¸‰äº•ä½å‹ãƒ•ã‚£ãƒŠãƒ³ã‚·ãƒ£ãƒ«ã‚°ãƒ«ãƒ¼ãƒ—",
        "8354": "ãµããŠã‹ãƒ•ã‚£ãƒŠãƒ³ã‚·ãƒ£ãƒ«ã‚°ãƒ«ãƒ¼ãƒ—",
        "8355": "é™å²¡éŠ€è¡Œ",
        "8411": "ã¿ãšã»ãƒ•ã‚£ãƒŠãƒ³ã‚·ãƒ£ãƒ«ã‚°ãƒ«ãƒ¼ãƒ—",
        "8601": "å¤§å’Œè¨¼åˆ¸ã‚°ãƒ«ãƒ¼ãƒ—æœ¬ç¤¾",
        "8604": "é‡æ‘ãƒ›ãƒ¼ãƒ«ãƒ‡ã‚£ãƒ³ã‚°ã‚¹",
        "8628": "æ¾äº•è¨¼åˆ¸",
        "8630": "SOMPOãƒ›ãƒ¼ãƒ«ãƒ‡ã‚£ãƒ³ã‚°ã‚¹",
        "8725": "MS&ADã‚¤ãƒ³ã‚·ãƒ¥ã‚¢ãƒ©ãƒ³ã‚¹ã‚°ãƒ«ãƒ¼ãƒ—ãƒ›ãƒ¼ãƒ«ãƒ‡ã‚£ãƒ³ã‚°ã‚¹",
        "8750": "ç¬¬ä¸€ç”Ÿå‘½ãƒ›ãƒ¼ãƒ«ãƒ‡ã‚£ãƒ³ã‚°ã‚¹",
        "8766": "æ±äº¬æµ·ä¸Šãƒ›ãƒ¼ãƒ«ãƒ‡ã‚£ãƒ³ã‚°ã‚¹",
        "8801": "ä¸‰äº•ä¸å‹•ç”£",
        "8802": "ä¸‰è±åœ°æ‰€",
        "8830": "ä½å‹ä¸å‹•ç”£",
        "9001": "æ±æ­¦é‰„é“",
        "9005": "æ±æ€¥",
        "9007": "å°ç”°æ€¥é›»é‰„",
        "9008": "äº¬ç‹é›»é‰„",
        "9009": "äº¬æˆé›»é‰„",
        "9020": "æ±æ—¥æœ¬æ—…å®¢é‰„é“",
        "9021": "è¥¿æ—¥æœ¬æ—…å®¢é‰„é“",
        "9022": "æ±æµ·æ—…å®¢é‰„é“",
        "9104": "å•†èˆ¹ä¸‰äº•",
        "9107": "å·å´æ±½èˆ¹",
        "9202": "ANAãƒ›ãƒ¼ãƒ«ãƒ‡ã‚£ãƒ³ã‚°ã‚¹",
        "9301": "ä¸‰è±å€‰åº«",
        "9432": "æ—¥æœ¬é›»ä¿¡é›»è©±",
        "9433": "KDDI",
        "9434": "ã‚½ãƒ•ãƒˆãƒãƒ³ã‚¯",
        "9501": "æ±äº¬é›»åŠ›ãƒ›ãƒ¼ãƒ«ãƒ‡ã‚£ãƒ³ã‚°ã‚¹",
        "9502": "ä¸­éƒ¨é›»åŠ›",
        "9503": "é–¢è¥¿é›»åŠ›",
        "9531": "æ±äº¬ã‚¬ã‚¹",
        "9532": "å¤§é˜ªã‚¬ã‚¹",
        "9602": "æ±å®",
        "9681": "æ±äº¬ãƒ‰ãƒ¼ãƒ ",
        "9684": "ã‚¹ã‚¯ã‚¦ã‚§ã‚¢ãƒ»ã‚¨ãƒ‹ãƒƒã‚¯ã‚¹ãƒ»ãƒ›ãƒ¼ãƒ«ãƒ‡ã‚£ãƒ³ã‚°ã‚¹",
        "9697": "ã‚«ãƒ—ã‚³ãƒ³",
        "9706": "æ—¥æœ¬ç©ºæ¸¯ãƒ“ãƒ«ãƒ‡ãƒ³ã‚°",
        "9719": "SCSK",
        "9735": "ã‚»ã‚³ãƒ ",
        "9766": "ã‚³ãƒŠãƒŸãƒ›ãƒ¼ãƒ«ãƒ‡ã‚£ãƒ³ã‚°ã‚¹",
        "9983": "ãƒ•ã‚¡ãƒ¼ã‚¹ãƒˆãƒªãƒ†ã‚¤ãƒªãƒ³ã‚°",
        "9984": "ã‚½ãƒ•ãƒˆãƒãƒ³ã‚¯ã‚°ãƒ«ãƒ¼ãƒ—",
    }
    
    return japanese_names.get(symbol_clean)


@st.cache_data(ttl=86400, show_spinner=False)  # 24æ™‚é–“ã‚­ãƒ£ãƒƒã‚·ãƒ¥
def get_japanese_company_name_cached(symbol: str, yfinance_info: Optional[Dict] = None) -> Optional[str]:
    """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä»˜ãã®æ—¥æœ¬èªç¤¾åå–å¾—é–¢æ•°"""
    return get_japanese_company_name(symbol, yfinance_info)


@st.cache_data(ttl=3600, show_spinner=False)  # 1æ™‚é–“ã‚­ãƒ£ãƒƒã‚·ãƒ¥
def fetch_article_content(url: str, timeout: int = 10) -> Optional[str]:
    """ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ã®URLã‹ã‚‰è¨˜äº‹ã®å…¨æ–‡ã‚’å–å¾—ã™ã‚‹ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ä»˜ãï¼‰"""
    if not SCRAPING_AVAILABLE or not url:
        return None
    
    try:
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
        }
        
        response = requests.get(url, headers=headers, timeout=timeout)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.content, "lxml")
        
        # ä¸€èˆ¬çš„ãªãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ã®æœ¬æ–‡ã‚»ãƒ¬ã‚¯ã‚¿ã‚’è©¦è¡Œ
        # æ—¥æœ¬èªãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚µã‚¤ãƒˆå‘ã‘ã®ã‚»ãƒ¬ã‚¯ã‚¿
        article_selectors = [
            'article',
            '.article-body',
            '.article-content',
            '.article-text',
            '.news-body',
            '.news-content',
            '.content-body',
            '#article-body',
            '#article-content',
            '#main-content',
            'main article',
            '[role="article"]',
            '.post-content',
            '.entry-content',
            'div.article',
            'div.content',
        ]
        
        article_text = None
        for selector in article_selectors:
            article_elem = soup.select_one(selector)
            if article_elem:
                # ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚„ã‚¹ã‚¿ã‚¤ãƒ«ã‚¿ã‚°ã‚’é™¤å»
                for script in article_elem(["script", "style", "nav", "header", "footer", "aside", "advertisement"]):
                    script.decompose()
                
                # ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—
                text = article_elem.get_text(separator="\n", strip=True)
                if text and len(text) > 100:  # æœ€ä½100æ–‡å­—ä»¥ä¸Šã‚ã‚‹ã“ã¨ã‚’ç¢ºèª
                    article_text = text
                    break
        
        # ã‚»ãƒ¬ã‚¯ã‚¿ã§è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã€pã‚¿ã‚°ã‚’é›†ã‚ã¦æœ¬æ–‡ã¨ã—ã¦ä½¿ç”¨
        if not article_text:
            paragraphs = soup.find_all("p")
            if paragraphs:
                text_parts = []
                for p in paragraphs:
                    text = p.get_text(strip=True)
                    if text and len(text) > 20:  # çŸ­ã™ãã‚‹æ®µè½ã¯é™¤å¤–
                        text_parts.append(text)
                if text_parts:
                    article_text = "\n".join(text_parts)
        
        # å–å¾—ã—ãŸãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        if article_text:
            # ä½™åˆ†ãªç©ºç™½ã‚’å‰Šé™¤
            lines = [line.strip() for line in article_text.split("\n") if line.strip()]
            article_text = "\n".join(lines)
            
            # æœ€ä½200æ–‡å­—ä»¥ä¸Šã‚ã‚‹ã“ã¨ã‚’ç¢ºèªï¼ˆsnippetã‚ˆã‚Šé•·ã„ã“ã¨ã‚’ä¿è¨¼ï¼‰
            if len(article_text) > 200:
                return article_text
        
        return None
    except Exception as e:
        logging.debug(f"è¨˜äº‹å…¨æ–‡å–å¾—å¤±æ•— ({url}): {e}")
        return None


@st.cache_data(ttl=1800, show_spinner=False)
def fetch_news(query: str, symbol: Optional[str] = None, max_results: int = 15, yfinance_info: Optional[Dict] = None) -> List[Dict]:
    """æ—¥æœ¬èªã®æœ€æ–°ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’ç¢ºå®Ÿã«å–å¾—ã™ã‚‹é–¢æ•°ï¼ˆæœ€ä½ä»¶æ•°ãŒå¾—ã‚‰ã‚Œã‚‹ã¾ã§å†è©¦è¡Œï¼‰"""
    if not query:
        return []
    
    # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€
    config = NEWS_SEARCH_CONFIG.get("search", {})
    keywords_config = NEWS_SEARCH_CONFIG.get("keywords", {})
    filtering_config = NEWS_SEARCH_CONFIG.get("filtering", {})
    
    # æ¤œç´¢ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    default_max_results = config.get("max_results", 15)
    max_results = max_results if max_results != 15 else default_max_results
    min_required_results = config.get("min_required_results", 5)
    max_retries = config.get("max_retries", 3)
    retry_delay_seconds = config.get("retry_delay_seconds", 2)
    multipliers = config.get("multipliers", {})
    min_candidates = config.get("min_candidates", {})
    timeout = config.get("timeout", 30)
    article_fetch_timeout = config.get("article_fetch_timeout", 15)
    
    # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
    japanese_search_templates = keywords_config.get("japanese_search_templates", [])
    japanese_symbol_templates = keywords_config.get("japanese_symbol_templates", [])
    japanese_combined_templates = keywords_config.get("japanese_combined_templates", [])
    english_search_templates = keywords_config.get("english_search_templates", [])
    
    # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    date_threshold_days = filtering_config.get("date_threshold_days", 365)
    focus_filter_config = filtering_config.get("focus_score", {})
    min_importance_score_when_focus_zero = focus_filter_config.get("min_importance_score_when_focus_zero", 4)
    fallback_sufficient_threshold_multiplier = filtering_config.get("fallback_sufficient_threshold_multiplier", 2)
    
    # æ—¥æœ¬æ ªã‹ã©ã†ã‹ã‚’åˆ¤å®šï¼ˆ.Tã§çµ‚ã‚ã‚‹ã€ã¾ãŸã¯4æ¡ã®æ•°å­—ï¼‰
    is_japanese_stock = False
    symbol_clean = None
    if symbol:
        symbol_upper = symbol.upper().strip()
        if symbol_upper.endswith(".T") or (symbol_upper.isdigit() and 4 <= len(symbol_upper) <= 5):
            is_japanese_stock = True
            symbol_clean = symbol.replace(".T", "").strip()
    
    # æ—¥æœ¬èªã®ç¤¾åã‚’å–å¾—ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ä»˜ãã€yfinance_infoã‚’æ¸¡ã™ï¼‰
    japanese_company_name = None
    if is_japanese_stock and symbol_clean:
        japanese_company_name = get_japanese_company_name_cached(symbol, yfinance_info)
    
    news_items = []
    seen_urls = set()  # é‡è¤‡ãƒã‚§ãƒƒã‚¯ç”¨
    errors = []  # ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°ç”¨
    
    # æœ€ä½ä»¶æ•°ãŒå¾—ã‚‰ã‚Œã‚‹ã¾ã§å†è©¦è¡Œã™ã‚‹
    for retry_attempt in range(max_retries):
        if retry_attempt > 0:
            # å†è©¦è¡Œå‰ã«å°‘ã—å¾…æ©Ÿï¼ˆAPIãƒ¬ãƒ¼ãƒˆåˆ¶é™ã‚’é¿ã‘ã‚‹ãŸã‚ï¼‰
            time.sleep(retry_delay_seconds)
            logging.info(f"ãƒ‹ãƒ¥ãƒ¼ã‚¹å–å¾—ã®å†è©¦è¡Œ {retry_attempt}/{max_retries - 1}ï¼ˆç¾åœ¨ã®ä»¶æ•°: {len(news_items)}ï¼‰")
        
        # æ—¥æœ¬æ ªã®å ´åˆã¯æ—¥æœ¬èªã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’å„ªå…ˆçš„ã«å–å¾—
        if is_japanese_stock:
            # æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®ãƒªã‚¹ãƒˆã‚’æ§‹ç¯‰
            search_keywords = []
            
            # æ—¥æœ¬èªã®ç¤¾åãŒã‚ã‚‹å ´åˆã¯ã€ãã‚Œã‚’å„ªå…ˆçš„ã«ä½¿ç”¨
            if japanese_company_name:
                for template in japanese_search_templates:
                    search_keywords.append(template.format(company_name=japanese_company_name))
            
            # ãƒ†ã‚£ãƒƒã‚«ãƒ¼ã‚·ãƒ³ãƒœãƒ«ã§ã®æ¤œç´¢ã‚‚è¿½åŠ 
            if symbol_clean and symbol_clean.isdigit():
                for template in japanese_symbol_templates:
                    search_keywords.append(template.format(symbol=symbol_clean))
                # æ—¥æœ¬èªã®ç¤¾åãŒã‚ã‚‹å ´åˆã¯ã€ãƒ†ã‚£ãƒƒã‚«ãƒ¼ã‚·ãƒ³ãƒœãƒ«ã¨çµ„ã¿åˆã‚ã›ãŸæ¤œç´¢ã‚‚è¿½åŠ 
                if japanese_company_name:
                    for template in japanese_combined_templates:
                        search_keywords.append(template.format(symbol=symbol_clean, company_name=japanese_company_name))
            
            # è‹±èªã®ç¤¾åã‚‚æ¤œç´¢ã«å«ã‚ã‚‹ï¼ˆæ—¥æœ¬èªãƒ‹ãƒ¥ãƒ¼ã‚¹ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
            for template in japanese_search_templates:
                search_keywords.append(template.format(company_name=query))
            
            # è¤‡æ•°ã®æ¤œç´¢ã‚’è©¦è¡Œ
            initial_multiplier = multipliers.get("initial_japanese", 8)
            initial_min_candidates = min_candidates.get("initial_japanese", 50)
            for idx, keywords in enumerate(search_keywords):
                # æ—¢ã«ååˆ†ãªä»¶æ•°ãŒå¾—ã‚‰ã‚Œã¦ã„ã‚‹å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
                if len(news_items) >= min_required_results * 2:
                    break
                
                # ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã‚’é¿ã‘ã‚‹ãŸã‚ã€æ¤œç´¢ã®é–“ã«å°‘ã—å¾…æ©Ÿï¼ˆæœ€åˆã®æ¤œç´¢ä»¥å¤–ï¼‰
                if idx > 0:
                    time.sleep(1)
                    
                try:
                    # timeoutãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯ddgsã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã«ã‚ˆã£ã¦ã¯ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ãªã„å¯èƒ½æ€§ãŒã‚ã‚‹
                    try:
                        ddgs_context = DDGS(timeout=timeout)
                    except (TypeError, ValueError):
                        # timeoutãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ãªã„å ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚’ä½¿ç”¨
                        ddgs_context = DDGS()
                    
                    with ddgs_context as ddgs:
                        japanese_results = list(
                            ddgs.news(
                                keywords=keywords,
                                region="jp-ja",
                                safesearch="Off",
                                max_results=max(max_results * initial_multiplier, initial_min_candidates),
                            )
                        )
                        for item in japanese_results:
                            url = item.get("url", "")
                            title = item.get("title", "")
                            if url and url not in seen_urls and title:
                                seen_urls.add(url)
                                news_items.append(
                                    {
                                        "title": title,
                                        "url": url,
                                        "snippet": item.get("body") or item.get("snippet") or "",
                                        "published": item.get("date"),
                                        "source": item.get("source") or "",
                                        "language": "ja",
                                    }
                                )
                except Exception as e:
                    error_str = str(e)
                    # ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯ç‰¹åˆ¥ãªå‡¦ç†
                    if "202" in error_str or "ratelimit" in error_str.lower() or "rate limit" in error_str.lower():
                        error_msg = f"æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ '{keywords}' ã§ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚ã—ã°ã‚‰ãå¾…ã£ã¦ã‹ã‚‰å†è©¦è¡Œã—ã¦ãã ã•ã„ã€‚"
                        errors.append(error_msg)
                        logging.warning(error_msg)
                        # ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã®å ´åˆã¯å°‘ã—é•·ã‚ã«å¾…æ©Ÿ
                        if retry_attempt < max_retries - 1:
                            time.sleep(retry_delay_seconds * 2)
                    else:
                        error_msg = f"æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ '{keywords}' ã§ã‚¨ãƒ©ãƒ¼: {error_str}"
                        errors.append(error_msg)
                        logging.warning(error_msg)
                    continue
            
            # æ—¢ã«ååˆ†ãªä»¶æ•°ãŒå¾—ã‚‰ã‚Œã¦ã„ã‚‹å ´åˆã¯ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ¤œç´¢ã‚’ã‚¹ã‚­ãƒƒãƒ—
            if len(news_items) >= min_required_results * 2:
                pass  # æ¬¡ã®å‡¦ç†ã«é€²ã‚€
            # æ—¥æœ¬èªãƒ‹ãƒ¥ãƒ¼ã‚¹ãŒå°‘ãªã„å ´åˆã€ã‚ˆã‚Šåºƒç¯„å›²ãªæ¤œç´¢ã‚’è©¦è¡Œ
            elif len(news_items) < min_required_results:
                fallback_queries = []
                if japanese_company_name:
                    fallback_queries.append(japanese_company_name)
                if symbol_clean and symbol_clean.isdigit():
                    fallback_queries.append(symbol_clean)
                fallback_queries.append(query)
                
                fallback_multiplier = multipliers.get("fallback_japanese", 4)
                fallback_min_candidates = min_candidates.get("fallback_japanese", 30)
                for idx, fallback_query in enumerate(fallback_queries):
                    # æ—¢ã«ååˆ†ãªä»¶æ•°ãŒå¾—ã‚‰ã‚Œã¦ã„ã‚‹å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
                    if len(news_items) >= min_required_results * 2:
                        break
                    
                    # ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã‚’é¿ã‘ã‚‹ãŸã‚ã€æ¤œç´¢ã®é–“ã«å°‘ã—å¾…æ©Ÿï¼ˆæœ€åˆã®æ¤œç´¢ä»¥å¤–ï¼‰
                    if idx > 0:
                        time.sleep(1)
                        
                    try:
                        try:
                            ddgs_context = DDGS(timeout=timeout)
                        except (TypeError, ValueError):
                            ddgs_context = DDGS()
                        
                        with ddgs_context as ddgs:
                            # ã‚ˆã‚Šã‚·ãƒ³ãƒ—ãƒ«ãªæ¤œç´¢ã‚¯ã‚¨ãƒªã§å†è©¦è¡Œ
                            fallback_results = list(
                                ddgs.news(
                                    keywords=fallback_query,
                                    region="jp-ja",
                                    safesearch="Off",
                                    max_results=max(max_results * fallback_multiplier, fallback_min_candidates),
                                )
                            )
                            for item in fallback_results:
                                url = item.get("url", "")
                                title = item.get("title", "")
                                if url and url not in seen_urls and title:
                                    seen_urls.add(url)
                                    news_items.append(
                                        {
                                            "title": title,
                                            "url": url,
                                            "snippet": item.get("body") or item.get("snippet") or "",
                                            "published": item.get("date"),
                                            "source": item.get("source") or "",
                                            "language": "ja",
                                        }
                                    )
                            # ååˆ†ãªãƒ‹ãƒ¥ãƒ¼ã‚¹ãŒå–å¾—ã§ããŸå ´åˆã¯ãƒ«ãƒ¼ãƒ—ã‚’æŠœã‘ã‚‹
                            if len(news_items) >= max_results * fallback_sufficient_threshold_multiplier:
                                break
                    except Exception as e:
                        error_str = str(e)
                        # ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯ç‰¹åˆ¥ãªå‡¦ç†
                        if "202" in error_str or "ratelimit" in error_str.lower() or "rate limit" in error_str.lower():
                            error_msg = f"ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ¤œç´¢ï¼ˆ'{fallback_query}'ï¼‰ã§ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚ã—ã°ã‚‰ãå¾…ã£ã¦ã‹ã‚‰å†è©¦è¡Œã—ã¦ãã ã•ã„ã€‚"
                            errors.append(error_msg)
                            logging.warning(error_msg)
                            # ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã®å ´åˆã¯å°‘ã—é•·ã‚ã«å¾…æ©Ÿ
                            if retry_attempt < max_retries - 1:
                                time.sleep(retry_delay_seconds * 2)
                        else:
                            error_msg = f"ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ¤œç´¢ï¼ˆ'{fallback_query}'ï¼‰ã§ã‚¨ãƒ©ãƒ¼: {error_str}"
                            errors.append(error_msg)
                            logging.warning(error_msg)
                        continue
        
        # æ—¥æœ¬æ ªã§ãªã„å ´åˆã€ã¾ãŸã¯æ—¥æœ¬èªãƒ‹ãƒ¥ãƒ¼ã‚¹ãŒå°‘ãªã„å ´åˆã¯è‹±èªã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚‚å–å¾—
        if not is_japanese_stock or len(news_items) < min_required_results:
            # é‡è¦åº¦ã®é«˜ã„ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’å„ªå…ˆçš„ã«å–å¾—ã™ã‚‹ãŸã‚ã®æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
            english_keywords = []
            for template in english_search_templates:
                english_keywords.append(template.format(query=query))
            
            english_multiplier = multipliers.get("english", 5)
            english_min_candidates = min_candidates.get("english", 30)
            for idx, keywords in enumerate(english_keywords):
                # æ—¢ã«ååˆ†ãªä»¶æ•°ãŒå¾—ã‚‰ã‚Œã¦ã„ã‚‹å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
                if len(news_items) >= min_required_results * 2:
                    break
                
                # ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã‚’é¿ã‘ã‚‹ãŸã‚ã€æ¤œç´¢ã®é–“ã«å°‘ã—å¾…æ©Ÿï¼ˆæœ€åˆã®æ¤œç´¢ä»¥å¤–ï¼‰
                if idx > 0:
                    time.sleep(1)
                    
                try:
                    try:
                        ddgs_context = DDGS(timeout=timeout)
                    except (TypeError, ValueError):
                        ddgs_context = DDGS()
                    
                    with ddgs_context as ddgs:
                        english_results = list(
                            ddgs.news(
                                keywords=keywords,
                                region="us-en",
                                safesearch="Off",
                                max_results=max(max_results * english_multiplier, english_min_candidates),
                            )
                        )
                        for item in english_results:
                            url = item.get("url", "")
                            title = item.get("title", "")
                            if url and url not in seen_urls and title:
                                seen_urls.add(url)
                                news_items.append(
                                    {
                                        "title": title,
                                        "url": url,
                                        "snippet": item.get("body") or item.get("snippet") or "",
                                        "published": item.get("date"),
                                        "source": item.get("source") or "",
                                        "language": "en",
                                    }
                                )
                except Exception as e:
                    error_str = str(e)
                    # ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯ç‰¹åˆ¥ãªå‡¦ç†
                    if "202" in error_str or "ratelimit" in error_str.lower() or "rate limit" in error_str.lower():
                        error_msg = f"æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ '{keywords}' ã§ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚ã—ã°ã‚‰ãå¾…ã£ã¦ã‹ã‚‰å†è©¦è¡Œã—ã¦ãã ã•ã„ã€‚"
                        errors.append(error_msg)
                        logging.warning(error_msg)
                        # ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã®å ´åˆã¯å°‘ã—é•·ã‚ã«å¾…æ©Ÿ
                        if retry_attempt < max_retries - 1:
                            time.sleep(retry_delay_seconds * 2)
                    else:
                        error_msg = f"æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ '{keywords}' ã§ã‚¨ãƒ©ãƒ¼: {error_str}"
                        errors.append(error_msg)
                        logging.warning(error_msg)
                    continue
        
        # æœ€ä½ä»¶æ•°ã«é”ã—ãŸå ´åˆã¯ãƒ«ãƒ¼ãƒ—ã‚’æŠœã‘ã‚‹
        if len(news_items) >= min_required_results:
            break
    
    # å†è©¦è¡Œå¾Œã®æœ€çµ‚çš„ãªä»¶æ•°ã‚’ãƒ­ã‚°ã«è¨˜éŒ²
    if len(news_items) < min_required_results:
        logging.warning(f"æœ€ä½ä»¶æ•°ï¼ˆ{min_required_results}ä»¶ï¼‰ã«é”ã—ã¾ã›ã‚“ã§ã—ãŸã€‚å–å¾—ä»¶æ•°: {len(news_items)}ä»¶")
    else:
        logging.info(f"ãƒ‹ãƒ¥ãƒ¼ã‚¹å–å¾—æˆåŠŸ: {len(news_items)}ä»¶ï¼ˆç›®æ¨™: {min_required_results}ä»¶ä»¥ä¸Šï¼‰")
    
    # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆã¯ãƒ­ã‚°ã«è¨˜éŒ²
    if errors and len(news_items) == 0:
        logging.error(f"ãƒ‹ãƒ¥ãƒ¼ã‚¹å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸã€‚ã‚¨ãƒ©ãƒ¼æ•°: {len(errors)}")
        for err in errors[:3]:  # æœ€åˆã®3ã¤ã®ã‚¨ãƒ©ãƒ¼ã®ã¿è¡¨ç¤º
            logging.error(err)
    
    # æœ€æ–°ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®ã¿ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼ˆè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®æ—¥æ•°ä»¥å†…ï¼‰
    news_items = filter_recent_news(news_items, days_threshold=date_threshold_days)
    
    # è–„ã„è¨˜äº‹ï¼ˆãƒ©ãƒ³ã‚­ãƒ³ã‚°ã‚„å¸‚å ´å‹•å‘è¨˜äº‹ï¼‰ã‚’é™¤å¤–
    filtered_news_items = []
    shallow_count = 0
    for item in news_items:
        if is_shallow_article(item, japanese_company_name, symbol):
            shallow_count += 1
            continue
        filtered_news_items.append(item)
    
    news_items = filtered_news_items
    
    # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°çµæœã‚’ãƒ­ã‚°ã«è¨˜éŒ²
    if shallow_count > 0:
        logging.info(f"è–„ã„è¨˜äº‹ã‚’ {shallow_count} ä»¶é™¤å¤–ã—ã¾ã—ãŸã€‚")
    
    # é‡è¦åº¦ã€ç„¦ç‚¹åº¦ã€æ—¥ä»˜ã§ã‚½ãƒ¼ãƒˆï¼ˆé‡è¦åº¦ã¨ç„¦ç‚¹åº¦ãŒé«˜ã„é †ã€åŒã˜ãªã‚‰æ–°ã—ã„é †ï¼‰
    news_items = sort_news_by_importance_and_date(
        news_items, 
        reverse=True,
        company_name=japanese_company_name or query,
        symbol=symbol,
        query=query
    )
    
    # ç„¦ç‚¹åº¦ãŒä½ã„è¨˜äº‹ã‚’é™¤å¤–ï¼ˆç„¦ç‚¹åº¦ã‚¹ã‚³ã‚¢ãŒ0ã®è¨˜äº‹ã¯é™¤å¤–ï¼‰
    # ãŸã ã—ã€é‡è¦åº¦ãŒé«˜ã„è¨˜äº‹ï¼ˆæ±ºç®—ç™ºè¡¨ãªã©ï¼‰ã¯ä¾‹å¤–ã¨ã—ã¦å«ã‚ã‚‹
    focus_filtered_items = []
    low_focus_count = 0
    for item in news_items:
        focus_score = calculate_focus_score(item, japanese_company_name or query, symbol, query)
        importance_score = calculate_news_importance_score(item)
        
        # ç„¦ç‚¹åº¦ãŒ0ã‹ã¤é‡è¦åº¦ã‚‚ä½ã„å ´åˆã¯é™¤å¤–ï¼ˆè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®é–¾å€¤ã‚’ä½¿ç”¨ï¼‰
        if focus_score == 0 and importance_score < min_importance_score_when_focus_zero:
            low_focus_count += 1
            continue
        focus_filtered_items.append(item)
    
    news_items = focus_filtered_items
    
    # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°çµæœã‚’ãƒ­ã‚°ã«è¨˜éŒ²
    if low_focus_count > 0:
        logging.info(f"ç„¦ç‚¹åº¦ã®ä½ã„è¨˜äº‹ã‚’ {low_focus_count} ä»¶é™¤å¤–ã—ã¾ã—ãŸã€‚")
    
    # max_resultsã¾ã§ã«åˆ¶é™ï¼ˆãŸã ã—ã€é‡è¦åº¦ã¨ç„¦ç‚¹åº¦ã®é«˜ã„ãƒ‹ãƒ¥ãƒ¼ã‚¹ã¯å„ªå…ˆçš„ã«å«ã‚ã‚‹ï¼‰
    news_items = news_items[:max_results]
    
    # å„ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚¢ã‚¤ãƒ†ãƒ ã«å¯¾ã—ã¦è¨˜äº‹ã®å…¨æ–‡ã‚’å–å¾—ï¼ˆsnippetãŒé€”ä¸­ã§åˆ‡ã‚Œã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ãŸã‚ï¼‰
    # å…¨æ–‡å–å¾—ã«å¤±æ•—ã—ãŸå ´åˆã¯ã€å…ƒã®snippetã‚’ä½¿ç”¨
    for news_item in news_items:
        url = news_item.get("url", "")
        original_snippet = news_item.get("snippet", "")
        
        # è¨˜äº‹ã®å…¨æ–‡ã‚’å–å¾—
        if url and original_snippet:
            full_content = fetch_article_content(url, timeout=article_fetch_timeout)
            if full_content:
                # å…¨æ–‡ãŒå–å¾—ã§ããŸå ´åˆã¯ã€snippetã‚’å…¨æ–‡ã§ç½®ãæ›ãˆ
                news_item["snippet"] = full_content
                news_item["full_content_fetched"] = True
            else:
                # å…¨æ–‡ãŒå–å¾—ã§ããªã‹ã£ãŸå ´åˆã¯ã€å…ƒã®snippetã‚’ä½¿ç”¨
                news_item["full_content_fetched"] = False
    
    return news_items


def build_analysis_payload(snapshot: Dict, news_items: List[Dict]) -> Dict:
    symbol_for_payload = snapshot.get("resolved_symbol") or snapshot.get("symbol")
    return {
        "symbol": symbol_for_payload,
        "company_name": snapshot["company_name"],
        "currency": snapshot["currency"],
        "price": snapshot["price"],
        "day_change_pct": snapshot["day_change_pct"],
        "analyst": snapshot["analyst"],
        "metrics": snapshot["key_metrics"],
        "news": news_items,
        "timestamp": snapshot["market_time"],
    }


def heuristic_analysis(snapshot: Dict) -> Dict:
    analyst = snapshot["analyst"]
    target_gap = analyst.get("target_gap_pct") or 0
    day_move = snapshot.get("day_change_pct") or 0
    reco = (analyst.get("recommendation_key") or "").lower()

    score = 55
    score += max(min(target_gap * 0.6, 20), -20)
    score -= max(min(abs(day_move) * 0.3, 10), 0) * (1 if day_move < 0 else -0.5)

    sentiment_bonus = {
        "strong_buy": 12,
        "buy": 8,
        "hold": 0,
        "sell": -10,
        "strong_sell": -15,
    }.get(reco, 0)
    score += sentiment_bonus
    score = max(0, min(100, round(score)))

    if score >= 66:
        action = "Buy"
        verdict = "æŠ¼ã—ç›®è²·ã„å¥½æ©Ÿ"
    elif score <= 40:
        action = "Sell"
        verdict = "ãƒªã‚¹ã‚¯å›é¿ã‚’å„ªå…ˆ"
    else:
        action = "Hold"
        verdict = "ä¸­ç«‹ï¼šæ§˜å­è¦‹"

    bullets = []
    if analyst.get("target_mean_price") and snapshot.get("price"):
        gap = analyst["target_gap_pct"]
        bullets.append(f"ç›®æ¨™æ ªä¾¡ã¾ã§ {format_percent(gap)} ã®ä½™åœ°")
    if reco:
        bullets.append(f"ã‚¢ãƒŠãƒªã‚¹ãƒˆè©•ä¾¡: {reco.upper()}")
    if snapshot["key_metrics"].get("trailingPE"):
        bullets.append(f"PER {snapshot['key_metrics']['trailingPE']:.1f}å€ã§å–å¼•ä¸­")
    while len(bullets) < 3:
        bullets.append("å¸‚å ´ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ã«å‚™ãˆã¦åˆ†æ•£ã‚’ç¶­æŒ")

    scenario = {
        "bullish_case": "å¤–éƒ¨AIã‚­ãƒ¼æœªè¨­å®šã®ãŸã‚ã€ã‚·ãƒ³ãƒ—ãƒ«æŒ‡æ¨™ã§å¼·æ°—ã‚·ãƒŠãƒªã‚ªã‚’æ¨å®šã—ã¦ã„ã¾ã™ã€‚",
        "bearish_case": "çŸ­æœŸãƒ†ã‚¯ãƒ‹ã‚«ãƒ«ã®æŒ¯ã‚Œã«æ³¨æ„ã—ã¤ã¤ãƒ•ã‚¡ãƒ³ãƒ€æŒ‡æ¨™ã®ç¢ºèªãŒå¿…è¦ã§ã™ã€‚",
        "competitive_edge": "ç›®æ¨™æ ªä¾¡ã¨æ©Ÿé–¢æŠ•è³‡å®¶å‹•å‘ã‚’ä¸»è¦ãªæ‹ ã‚Šæ‰€ã¨ã—ã¦ã„ã¾ã™ã€‚",
    }

    return {
        "verdict_short": verdict,
        "action": action,
        "score": score,
        "bullet_points": bullets[:3],
        "scenario": scenario,
        "analysis_comment": "å¤–éƒ¨AIãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’å–å¾—ã§ããªã‹ã£ãŸãŸã‚çµ±è¨ˆãƒ™ãƒ¼ã‚¹ã®æš«å®šã‚³ãƒ¡ãƒ³ãƒˆã§ã™ã€‚",
        "source": "heuristic",
    }


def _sanitize_ai_response(parsed: Dict) -> Dict:
    parsed = parsed or {}
    parsed["bullet_points"] = (parsed.get("bullet_points") or [])[:3]
    return parsed


def parse_ai_json_payload(message: Optional[str]) -> Optional[Dict]:
    """Accept AI responses with code fences or extra text and extract JSON."""
    if not message:
        return None
    cleaned = message.strip()
    if cleaned.startswith("```"):
        cleaned = re.sub(r"^```(?:json)?", "", cleaned, flags=re.IGNORECASE).strip()
        cleaned = re.sub(r"```$", "", cleaned).strip()
    brace_start = cleaned.find("{")
    brace_end = cleaned.rfind("}")
    if brace_start != -1 and brace_end != -1:
        cleaned = cleaned[brace_start : brace_end + 1]
    try:
        return json.loads(cleaned)
    except json.JSONDecodeError:
        return None


def request_openai_analysis(api_key: Optional[str], payload: Dict) -> Optional[Dict]:
    api_key_clean = (api_key or "").strip()
    if not api_key_clean:
        return None
    try:
        client = OpenAI(api_key=api_key_clean)
        response = client.chat.completions.create(
            model=OPENAI_DEFAULT_MODEL,
            temperature=0.0,
            response_format={"type": "json_object"},
            messages=[
                {"role": "system", "content": AI_SYSTEM_PROMPT},
                {"role": "user", "content": build_ai_user_prompt(payload)},
            ],
        )
        message = response.choices[0].message.content if response.choices else None
        parsed = parse_ai_json_payload(message)
        if not parsed:
            return None
        parsed["source"] = "openai"
        return _sanitize_ai_response(parsed)
    except Exception as e:  # pragma: no cover - API failure
        st.error(f"OpenAI APIå‘¼ã³å‡ºã—ã‚¨ãƒ©ãƒ¼: {str(e)}")
        return None


def request_gemini_analysis(
    api_key: Optional[str],
    payload: Dict,
    model_name: Optional[str],
) -> Optional[Dict]:
    if genai is None:
        return None
    api_key_clean = (api_key or "").strip()
    if not api_key_clean:
        return None
    model_id = (model_name or DEFAULT_GEMINI_MODEL).strip() or DEFAULT_GEMINI_MODEL
    try:
        genai.configure(api_key=api_key_clean)
        model = genai.GenerativeModel(
            model_name=model_id,
            system_instruction=AI_SYSTEM_PROMPT,
        )
        response = model.generate_content(
            contents=build_ai_user_prompt(payload),
            generation_config={
                "temperature": 0.2,
                "response_mime_type": "application/json",
            },
        )
        message = getattr(response, "text", None)
        if not message and getattr(response, "candidates", None):
            first_candidate = response.candidates[0]
            content = getattr(first_candidate, "content", None)
            parts = getattr(content, "parts", None)
            if parts:
                first_part = parts[0]
                message = getattr(first_part, "text", None) or getattr(first_part, "content", None)
        if not message:
            return None
        parsed = parse_ai_json_payload(message)
        if not parsed:
            return None
        parsed["source"] = "gemini"
        return _sanitize_ai_response(parsed)
    except Exception as e:  # pragma: no cover - API failure
        st.error(f"Gemini APIå‘¼ã³å‡ºã—ã‚¨ãƒ©ãƒ¼: {str(e)}")
        return None


def generate_ai_analysis(
    openai_api_key: Optional[str],
    google_api_key: Optional[str],
    snapshot: Dict,
    news_items: List[Dict],
    google_model_name: Optional[str],
) -> Dict:
    payload = build_analysis_payload(snapshot, news_items)
    fallback = heuristic_analysis(snapshot)

    google_key_clean = (google_api_key or "").strip()
    if google_key_clean:
        google_response = request_gemini_analysis(google_key_clean, payload, google_model_name)
        if google_response:
            return google_response

    openai_key_clean = (openai_api_key or "").strip()
    if openai_key_clean:
        openai_response = request_openai_analysis(openai_key_clean, payload)
        if openai_response:
            return openai_response

    return fallback


def render_header(snapshot: Dict, analysis: Dict):
    price = snapshot.get("price")
    day_pct = snapshot.get("day_change_pct")
    day_abs = snapshot.get("day_change")
    currency = snapshot.get("currency", "USD")
    symbol_label = snapshot.get("display_symbol") or snapshot.get("symbol")

    day_class = "positive" if (day_pct or 0) >= 0 else "negative"
    day_text = (
        f"{format_currency(day_abs, currency)} ({format_percent(day_pct)})"
        if day_abs is not None
        else "â€”"
    )

    analyst = snapshot["analyst"]
    reco_key = analyst.get("recommendation_key")
    reco_mean = analyst.get("recommendation_mean")
    if reco_mean:
        converted = round(6 - float(reco_mean), 1)  # 1 (best) -> 5
        reco_text = f"{converted}/5"
    else:
        reco_text = "N/A"
    analyst_label = reco_key.upper() if reco_key else "N/A"

    header_html = f"""
    <div class="header-card">
        <div class="header-symbol">{symbol_label} Â· {snapshot['company_name']}</div>
        <div class="header-price">{format_currency(price, currency)}</div>
        <div class="price-change {day_class}">{day_text}</div>
        <div class="score-grid">
            <div class="score-card">
                <div class="score-label">AI æŠ•è³‡ã‚¹ã‚³ã‚¢</div>
                <div class="score-value">{analysis.get('score', 0)}/100</div>
            </div>
            <div class="score-card">
                <div class="score-label">ã‚¢ãƒŠãƒªã‚¹ãƒˆæ¨å¥¨</div>
                <div class="score-value">{analyst_label}<br/><span style="font-size:0.85rem;color:#9ca3af;">{reco_text}</span></div>
            </div>
        </div>
    </div>
    """
    st.markdown(header_html, unsafe_allow_html=True)


def render_conclusion(analysis: Dict):
    action = analysis.get("action", "Hold")
    verdict = analysis.get("verdict_short", "æƒ…å ±ä¸è¶³")
    bullets = analysis.get("bullet_points", [])
    bullet_html = "".join(f"<li>{point}</li>" for point in bullets)
    conclusion_html = f"""
    <div class="conclusion-card">
        <div class="action-pill action-{action}">{action}</div>
        <h2 style="margin:12px 0 6px 0;">{verdict}</h2>
        <ul class="bullet-list">{bullet_html}</ul>
    </div>
    """
    st.markdown(conclusion_html, unsafe_allow_html=True)


def describe_analysis_source(analysis: Dict) -> str:
    source = (analysis or {}).get("source")
    mapping = {
        "gemini": "Gemini (Google AI Studio)",
        "openai": "OpenAI GPT",
        "heuristic": "ãƒ’ãƒ¥ãƒ¼ãƒªã‚¹ãƒ†ã‚£ãƒƒã‚¯ï¼ˆAPIæœªä½¿ç”¨ï¼‰",
    }
    return mapping.get(source, "ãƒ’ãƒ¥ãƒ¼ãƒªã‚¹ãƒ†ã‚£ãƒƒã‚¯ï¼ˆAPIæœªä½¿ç”¨ï¼‰")


def render_tabs(analysis: Dict, snapshot: Dict, news_items: List[Dict]):
    tabs = st.tabs(["ã‚·ãƒŠãƒªã‚ª", "ãƒ—ãƒ­ã®è©•ä¾¡", "ãƒ‡ãƒ¼ã‚¿ / ãƒ‹ãƒ¥ãƒ¼ã‚¹", "rawãƒ‡ãƒ¼ã‚¿"])

    scenario = analysis.get("scenario", {})
    with tabs[0]:
        st.markdown("**Bullã‚·ãƒŠãƒªã‚ª**")
        st.write(scenario.get("bullish_case", "æƒ…å ±ä¸è¶³"))
        st.markdown("**Bearã‚·ãƒŠãƒªã‚ª**")
        st.write(scenario.get("bearish_case", "æƒ…å ±ä¸è¶³"))
        st.markdown("**ç«¶åˆå„ªä½æ€§ / Moat**")
        st.write(scenario.get("competitive_edge", "æƒ…å ±ä¸è¶³"))

    analyst = snapshot["analyst"]
    with tabs[1]:
        st.markdown("**ã‚¢ãƒŠãƒªã‚¹ãƒˆã‚³ãƒ³ã‚»ãƒ³ã‚µã‚¹**")
        st.markdown(
            f"- æ¨å¥¨: `{analyst.get('recommendation_key') or 'N/A'}`\n"
            f"- ã‚¢ãƒŠãƒªã‚¹ãƒˆæ•°: {analyst.get('opinion_count') or 'â€”'}å"
        )

        st.markdown("**ç›®æ¨™æ ªä¾¡ã‚®ãƒ£ãƒƒãƒ—**")
        st.write(
            f"å¹³å‡: {format_currency(analyst.get('target_mean_price'), snapshot['currency'])} "
            f"({format_percent(analyst.get('target_gap_pct'))})"
        )

        inst = analyst.get("institutional_ownership_pct")
        st.markdown(
            "**æ©Ÿé–¢æŠ•è³‡å®¶ä¿æœ‰æ¯”ç‡**: "
            f"{format_percent(inst) if inst is not None else 'ãƒ‡ãƒ¼ã‚¿ãªã—'}"
        )
        st.markdown("**AIã‚³ãƒ¡ãƒ³ãƒˆ vs ãƒ—ãƒ­**")
        st.write(analysis.get("analysis_comment", "â€”"))

    metrics = snapshot["key_metrics"]
    with tabs[2]:
        # æ ªä¾¡ã‚°ãƒ©ãƒ•ã‚»ã‚¯ã‚·ãƒ§ãƒ³
        st.markdown("**ğŸ“ˆ æ ªä¾¡ãƒãƒ£ãƒ¼ãƒˆ**")
        symbol = snapshot.get("symbol") or snapshot.get("resolved_symbol")
        currency = snapshot.get("currency", "USD")
        
        # æœŸé–“é¸æŠ
        period_options = {
            "1æ—¥": "1d",
            "5æ—¥": "5d",
            "1é€±é–“": "1wk",
            "1ãƒ¶æœˆ": "1mo",
            "3ãƒ¶æœˆ": "3mo",
            "6ãƒ¶æœˆ": "6mo",
            "1å¹´": "1y",
            "2å¹´": "2y",
            "5å¹´": "5y",
        }
        selected_period_label = st.selectbox(
            "æœŸé–“ã‚’é¸æŠ",
            options=list(period_options.keys()),
            index=3,  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯1ãƒ¶æœˆ
            key="stock_chart_period"
        )
        selected_period = period_options[selected_period_label]
        
        if symbol:
            with st.spinner("æ ªä¾¡ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ä¸­..."):
                history_data = fetch_stock_history(symbol, period=selected_period)
            
            if history_data.get("error"):
                st.error(f"æ ªä¾¡ãƒ‡ãƒ¼ã‚¿ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ: {history_data['error']}")
            else:
                # ã‚°ãƒ©ãƒ•ã‚’è¡¨ç¤º
                fig = create_stock_chart(history_data, symbol, currency)
                st.plotly_chart(fig, use_container_width=True)
                
                # ãƒ‡ãƒ¼ã‚¿æä¾›å…ƒã¸ã®ãƒªãƒ³ã‚¯
                yahoo_url = get_yahoo_finance_url(symbol)
                st.markdown(
                    f'<div style="text-align: center; margin-top: 10px;">'
                    f'<a href="{yahoo_url}" target="_blank" style="color: #3b82f6; text-decoration: none;">'
                    f'ğŸ“Š Yahoo Financeã§è©³ç´°ã‚’è¦‹ã‚‹</a></div>',
                    unsafe_allow_html=True
                )
                st.caption("ã‚°ãƒ©ãƒ•ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¦æ‹¡å¤§è¡¨ç¤ºã§ãã¾ã™ã€‚ãƒ‡ãƒ¼ã‚¿æä¾›å…ƒ: Yahoo Finance")
        else:
            st.warning("ã‚·ãƒ³ãƒœãƒ«æƒ…å ±ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
        
        st.divider()
        
        st.markdown("**ä¸»è¦æŒ‡æ¨™**")
        metric_lines = []
        pairs = [
            ("PER (TTM)", metrics.get("trailingPE")),
            ("PER (Forward)", metrics.get("forwardPE")),
            ("PEG", metrics.get("pegRatio")),
            ("PBR", metrics.get("priceToBook")),
            ("EPS (TTM)", metrics.get("trailingEps")),
            ("é…å½“åˆ©å›ã‚Š", metrics.get("dividendYield")),
            ("Beta", metrics.get("beta")),
            ("æ™‚ä¾¡ç·é¡", metrics.get("marketCap")),
        ]
        for label, value in pairs:
            if value is None:
                formatted = "â€”"
            elif "åˆ©å›ã‚Š" in label:
                formatted = format_percent(value)
            elif label == "æ™‚ä¾¡ç·é¡":
                formatted = f"${value/1_000_000_000:,.1f}B"
            else:
                formatted = f"{value:,.2f}" if isinstance(value, (float, int)) else value
            metric_lines.append(f"- {label}: {formatted}")
        st.markdown("\n".join(metric_lines))

        st.markdown("**é–¢é€£ãƒ‹ãƒ¥ãƒ¼ã‚¹**")
        if not news_items:
            st.warning("æœ€æ–°ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸã€‚ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ¥ç¶šã‚„æ¤œç´¢ã‚µãƒ¼ãƒ“ã‚¹ã®çŠ¶æ…‹ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        else:
            st.caption(f"ğŸ“° {len(news_items)} ä»¶ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’å–å¾—ã—ã¾ã—ãŸ")
        for news in news_items:
            st.markdown(
                f'<div class="news-item"><a class="news-title" href="{news["url"]}" target="_blank">{news["title"]}</a>'
                f'<div class="news-meta">{news.get("source") or ""} Â· {news.get("published") or ""}</div>'
                f'<div class="news-body">{news.get("snippet") or ""}</div></div>',
                unsafe_allow_html=True,
            )

    with tabs[3]:
        st.markdown("**ğŸ“‹ rawãƒ‡ãƒ¼ã‚¿**")
        st.caption("å–å¾—ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚’ãã®ã¾ã¾è¡¨ç¤ºã—ã¾ã™ã€‚ãƒ‡ãƒãƒƒã‚°ã‚„ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ”¹è‰¯ã®å‚è€ƒã«ã—ã¦ãã ã•ã„ã€‚")
        
        # åˆ†æç”¨ãƒšã‚¤ãƒ­ãƒ¼ãƒ‰ã‚’æ§‹ç¯‰
        payload = build_analysis_payload(snapshot, news_items)
        
        st.markdown("#### 1. æ ªä¾¡ãƒ»çµŒå–¶æŒ‡æ¨™ãƒ‡ãƒ¼ã‚¿ï¼ˆsnapshotï¼‰")
        st.json(snapshot)
        
        st.markdown("#### 2. ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ï¼ˆnews_itemsï¼‰")
        st.json(news_items)
        
        st.markdown("#### 3. AIåˆ†æç”¨ãƒšã‚¤ãƒ­ãƒ¼ãƒ‰ï¼ˆpayloadï¼‰")
        st.json(payload)
        
        st.markdown("#### 4. AIåˆ†æçµæœï¼ˆanalysisï¼‰")
        st.json(analysis)


def main():
    st.title("ğŸ“± Mobile AI Investment Dashboard")
    st.caption("å¿™ã—ã„ãƒ“ã‚¸ãƒã‚¹ãƒãƒ³å‘ã‘ã®å³æ–­æ”¯æ´ãƒ„ãƒ¼ãƒ«ï¼ˆå­¦ç¿’ç›®çš„ã®ã¿ï¼‰")

    google_model_input = st.text_input(
        "Gemini ãƒ¢ãƒ‡ãƒ«ID",
        value=DEFAULT_GEMINI_MODEL,
        help="APIã‚­ãƒ¼ã§æœ‰åŠ¹ãªãƒ¢ãƒ‡ãƒ«IDï¼ˆä¾‹: gemini-1.5-flashï¼‰ã‚’æŒ‡å®šã€‚Chrome ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ç®¡ç†ã§IDã¨ã—ã¦ä¿å­˜ã•ã‚Œã¾ã™ã€‚",
    )
    google_api_key_default = resolve_google_api_key_from_env()
    google_api_key = st.text_input(
        "Google AI Studio API Keyï¼ˆGemini / ä»»æ„ï¼‰",
        type="password",
        value=google_api_key_default,
        help="ç’°å¢ƒå¤‰æ•°ã‹ã‚‰è‡ªå‹•å…¥åŠ›ã•ã‚Œã‚‹ã»ã‹ã€Chrome ã®ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã«ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ã¨ã—ã¦ä¿å­˜ã§ãã¾ã™ã€‚",
    )
    google_model_name = (google_model_input or "").strip() or DEFAULT_GEMINI_MODEL
    
    openai_api_key_default = os.getenv("OPENAI_API_KEY", "")
    openai_api_key = st.text_input(
        "OpenAI API Keyï¼ˆä»»æ„ãƒ»ãƒ­ãƒ¼ã‚«ãƒ«ã§ä¿æŒï¼‰",
        type="password",
        value=openai_api_key_default,
        help="APIã‚­ãƒ¼ã¯ãƒ–ãƒ©ã‚¦ã‚¶å†…ã®ã¿ã§ä½¿ç”¨ã•ã‚Œã€Chrome ã®ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã«ä¿å­˜ã—ã¦è‡ªå‹•å…¥åŠ›ã§ãã¾ã™ã€‚",
    )
    
    ticker_input = st.text_input("ãƒ†ã‚£ãƒƒã‚«ãƒ¼ã‚·ãƒ³ãƒœãƒ«", value="6501")

    if "effective_openai_api_key" not in st.session_state:
        st.session_state["effective_openai_api_key"] = openai_api_key_default.strip()
    if "effective_google_api_key" not in st.session_state:
        st.session_state["effective_google_api_key"] = google_api_key_default.strip()
    if "effective_gemini_model" not in st.session_state:
        st.session_state["effective_gemini_model"] = google_model_name
    if "api_status_snapshot" not in st.session_state:
        st.session_state["api_status_snapshot"] = build_api_status_snapshot(
            st.session_state["effective_openai_api_key"],
            st.session_state["effective_google_api_key"],
            st.session_state["effective_gemini_model"],
        )

    enable_chrome_password_manager_support()
    st.markdown("#### ğŸ”„ APIã‚­ãƒ¼ã®é©ç”¨")
    st.caption("å…¥åŠ›ã—ãŸã‚­ãƒ¼ã‚’ã‚¢ãƒ—ãƒªã«åæ˜ ã—ã€ç”»é¢ä¸‹éƒ¨ã®ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹è¡¨ç¤ºã‚’æ›´æ–°ã—ã¾ã™ã€‚")
    apply_api_keys = st.button(
        "APIã‚­ãƒ¼ã‚’é©ç”¨ã—ã¦ç”»é¢ä¸‹éƒ¨ã‚’æ›´æ–°",
        type="primary",
        use_container_width=True,
        help="AI åˆ†æã§ä½¿ç”¨ã™ã‚‹ã‚­ãƒ¼ã‚’ç¢ºå®šã—ã€ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’æœ€æ–°åŒ–ã—ã¾ã™ã€‚",
    )
    if apply_api_keys:
        applied_openai = openai_api_key.strip()
        applied_google = google_api_key.strip()
        st.session_state["effective_openai_api_key"] = applied_openai
        st.session_state["effective_google_api_key"] = applied_google
        st.session_state["effective_gemini_model"] = google_model_name
        applied_timestamp = datetime.now(timezone.utc).strftime("%Y-%m-%d %H:%M:%S UTC")
        st.session_state["api_status_snapshot"] = build_api_status_snapshot(
            applied_openai,
            applied_google,
            google_model_name,
            applied_timestamp,
        )
        if applied_google:
            st.success(f"Google APIã‚­ãƒ¼ã‚’é©ç”¨ã—ã¾ã—ãŸï¼ˆé•·ã•: {len(applied_google)}æ–‡å­—ï¼‰ã€‚æ¬¡ã®åˆ†æã§ä½¿ç”¨ã•ã‚Œã¾ã™ã€‚")
        elif applied_openai:
            st.success(f"OpenAI APIã‚­ãƒ¼ã‚’é©ç”¨ã—ã¾ã—ãŸï¼ˆé•·ã•: {len(applied_openai)}æ–‡å­—ï¼‰ã€‚æ¬¡ã®åˆ†æã§ä½¿ç”¨ã•ã‚Œã¾ã™ã€‚")
        else:
            st.warning("APIã‚­ãƒ¼ãŒå…¥åŠ›ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚ãƒ’ãƒ¥ãƒ¼ãƒªã‚¹ãƒ†ã‚£ãƒƒã‚¯åˆ†æãŒä½¿ç”¨ã•ã‚Œã¾ã™ã€‚")

    effective_openai_key = st.session_state.get("effective_openai_api_key", "").strip()
    effective_google_key = st.session_state.get("effective_google_api_key", "").strip()
    effective_gemini_model = st.session_state.get("effective_gemini_model", google_model_name)

    if not ticker_input:
        st.info("åˆ†æã—ãŸã„ãƒ†ã‚£ãƒƒã‚«ãƒ¼ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
        return

    normalized = normalize_ticker_input(ticker_input)
    query_symbol = normalized.get("query_symbol")
    if not query_symbol:
        st.error("ãƒ†ã‚£ãƒƒã‚«ãƒ¼ã®å½¢å¼ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        return

    with st.spinner("ãƒãƒ¼ã‚±ãƒƒãƒˆãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ä¸­..."):
        snapshot = fetch_ticker_snapshot(query_symbol)

    if snapshot.get("error"):
        st.error(snapshot["error"])
        return

    snapshot["display_symbol"] = normalized.get("display_symbol") or snapshot.get("symbol")
    snapshot["input_symbol"] = normalized.get("input_symbol")
    snapshot["resolved_symbol"] = snapshot.get("symbol")
    if normalized.get("conversion_note"):
        st.caption(normalized["conversion_note"])

    with st.spinner("æœ€æ–°ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’å–å¾—ä¸­..."):
        # snapshotã‹ã‚‰infoã‚’å–å¾—ã—ã¦æ—¥æœ¬èªåå–å¾—ã«æ´»ç”¨
        yfinance_info = snapshot.get("info", {})
        news_items = fetch_news(snapshot["company_name"], symbol=snapshot.get("symbol"), yfinance_info=yfinance_info)
    
    # ãƒ‹ãƒ¥ãƒ¼ã‚¹å–å¾—çµæœã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯
    if not news_items:
        st.warning("âš ï¸ æœ€æ–°ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸã€‚ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ¥ç¶šã‚„æ¤œç´¢ã‚µãƒ¼ãƒ“ã‚¹ã®çŠ¶æ…‹ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
    elif len(news_items) < 3:
        st.info(f"â„¹ï¸ ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’ {len(news_items)} ä»¶å–å¾—ã—ã¾ã—ãŸï¼ˆç›®æ¨™: 5ä»¶ï¼‰ã€‚")
    
    # APIã‚­ãƒ¼ã®çŠ¶æ…‹ã‚’ç¢ºèª
    if effective_google_key:
        st.info(f"ğŸ”‘ Google APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã™ï¼ˆãƒ¢ãƒ‡ãƒ«: {effective_gemini_model}ï¼‰ã€‚Gemini APIã‚’ä½¿ç”¨ã—ã¦åˆ†æã—ã¾ã™ã€‚")
    elif effective_openai_key:
        st.info("ğŸ”‘ OpenAI APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã™ã€‚OpenAI APIã‚’ä½¿ç”¨ã—ã¦åˆ†æã—ã¾ã™ã€‚")
    else:
        st.warning("âš ï¸ APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚ãƒ’ãƒ¥ãƒ¼ãƒªã‚¹ãƒ†ã‚£ãƒƒã‚¯åˆ†æã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
    
    with st.spinner("AIãŒåˆ†æä¸­..."):
        analysis = generate_ai_analysis(
            effective_openai_key,
            effective_google_key,
            snapshot,
            news_items,
            effective_gemini_model,
        )

    render_header(snapshot, analysis)
    st.caption(f"AIã‚¨ãƒ³ã‚¸ãƒ³å‡ºåŠ›: {describe_analysis_source(analysis)}")
    st.markdown("### âœ… çµè«–ã‚¨ãƒªã‚¢")
    render_conclusion(analysis)
    st.markdown("### ğŸ“Š è©³ç´°ã‚¨ãƒªã‚¢")
    render_tabs(analysis, snapshot, news_items)
    render_api_status_panel(st.session_state.get("api_status_snapshot"))

    st.markdown(
        "<p class='disclaimer'>* æœ¬ã‚¢ãƒ—ãƒªã¯æ•™è‚²ç›®çš„ã®æƒ…å ±æä¾›ãƒ„ãƒ¼ãƒ«ã§ã™ã€‚æŠ•è³‡åˆ¤æ–­ã¯ã”è‡ªèº«ã®è²¬ä»»ã§è¡Œã£ã¦ãã ã•ã„ã€‚</p>",
        unsafe_allow_html=True,
    )


if __name__ == "__main__":
    main()
